<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>llm微调调研 | 5ha的个人空间</title><meta name="author" content="5ha"><meta name="copyright" content="5ha"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大模型训练与微调大语言模型（LLM）的成长过程，大体可以分为三个阶段：     阶段 目标 学习方式 类比     预训练（Pretraining） 学习语言规律和知识 自监督学习 小孩学习基础语法   监督微调（SFT） 学会听懂“人类指令” 人类示范样本 老师手把手教你回答问题   对齐训练（RLHF &#x2F; DPO） 学会“说得合适” 人类偏好反馈 学会在社交中说话得体      🧩 简单来理">
<meta property="og:type" content="article">
<meta property="og:title" content="llm微调调研">
<meta property="og:url" content="http://example.com/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/index.html">
<meta property="og:site_name" content="5ha的个人空间">
<meta property="og:description" content="大模型训练与微调大语言模型（LLM）的成长过程，大体可以分为三个阶段：     阶段 目标 学习方式 类比     预训练（Pretraining） 学习语言规律和知识 自监督学习 小孩学习基础语法   监督微调（SFT） 学会听懂“人类指令” 人类示范样本 老师手把手教你回答问题   对齐训练（RLHF &#x2F; DPO） 学会“说得合适” 人类偏好反馈 学会在社交中说话得体      🧩 简单来理">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/image-20251017134429782.png">
<meta property="article:published_time" content="2025-11-17T16:13:40.000Z">
<meta property="article:modified_time" content="2025-11-19T15:04:58.539Z">
<meta property="article:author" content="5ha">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/image-20251017134429782.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "llm微调调研",
  "url": "http://example.com/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/",
  "image": "http://example.com/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/image-20251017134429782.png",
  "datePublished": "2025-11-17T16:13:40.000Z",
  "dateModified": "2025-11-19T15:04:58.539Z",
  "author": [
    {
      "@type": "Person",
      "name": "5ha",
      "url": "http://example.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/avatar.jpg"><link rel="canonical" href="http://example.com/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'llm微调调研',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/background2.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于me</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/image-20251017134429782.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">5ha的个人空间</span></a><a class="nav-page-title" href="/"><span class="site-name">llm微调调研</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于me</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">llm微调调研</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-17T16:13:40.000Z" title="发表于 2025-11-18 00:13:40">2025-11-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-19T15:04:58.539Z" title="更新于 2025-11-19 23:04:58">2025-11-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="大模型训练与微调"><a href="#大模型训练与微调" class="headerlink" title="大模型训练与微调"></a>大模型训练与微调</h1><p>大语言模型（LLM）的成长过程，大体可以分为三个阶段：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>目标</th>
<th>学习方式</th>
<th>类比</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>预训练（Pretraining）</strong></td>
<td>学习语言规律和知识</td>
<td>自监督学习</td>
<td>小孩学习基础语法</td>
</tr>
<tr>
<td><strong>监督微调（SFT）</strong></td>
<td>学会听懂“人类指令”</td>
<td>人类示范样本</td>
<td>老师手把手教你回答问题</td>
</tr>
<tr>
<td><strong>对齐训练（RLHF / DPO）</strong></td>
<td>学会“说得合适”</td>
<td>人类偏好反馈</td>
<td>学会在社交中说话得体</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>🧩 简单来理解：<br>预训练让模型“有知识”；<br>微调让模型“能沟通”；<br>对齐让模型“合人意”。</p>
</blockquote>
<h2 id="一、预训练"><a href="#一、预训练" class="headerlink" title="一、预训练"></a>一、预训练</h2><h3 id="1-1-什么是预训练？"><a href="#1-1-什么是预训练？" class="headerlink" title="1.1 什么是预训练？"></a>1.1 什么是预训练？</h3><h4 id="1-1-1-背景与概念"><a href="#1-1-1-背景与概念" class="headerlink" title="1.1.1 背景与概念"></a>1.1.1 <strong>背景与概念</strong></h4><p> 在大语言模型（LLM, Large Language Model）中，<strong>预训练（Pre-training）</strong> 是整个模型训练流程的第一阶段，也是最关键的一步。它的目标是让模型在大规模、无标注的文本数据上，通过<strong>自监督学习</strong>（Self-supervised Learning）的方式，<strong>掌握语言的基础规律和世界常识</strong>。<br> 这些数据通常来源于互联网网页、新闻、百科、书籍、社交媒体文本、论文等，从而使模型具备理解自然语言、生成自然语言的通用能力。</p>
<h4 id="1-1-2-类比理解"><a href="#1-1-2-类比理解" class="headerlink" title="1.1.2 类比理解"></a>1.1.2 <strong>类比理解</strong></h4><p> 可以把预训练后的模型比作刚刚毕业的大学生——他们已经通过大学阶段的“通识教育”，掌握了丰富的理论知识与基本技能，具备了在社会中工作的基础能力。但要真正胜任某个具体岗位（比如程序员、医生、律师），还需要进行“上岗培训”和“岗位实战”，也就是模型在预训练之后所需要的<strong>微调（Fine-tuning）</strong>和<strong>对齐（Alignment）</strong>阶段。</p>
<blockquote>
<p>换句话说，<strong>预训练是让模型“学会语言”</strong>，而微调和对齐则是让它“懂人话”、“会办事”。</p>
</blockquote>
<h4 id="1-1-3-核心目标"><a href="#1-1-3-核心目标" class="headerlink" title="1.1.3 核心目标"></a>1.1.3 <strong>核心目标</strong></h4><p> 通过预训练，模型能够在以下方面获得能力：</p>
<ul>
<li><strong>语言规律</strong>：掌握语法、句法结构、上下文逻辑；</li>
<li><strong>语义理解</strong>：理解词语和句子的深层含义；</li>
<li><strong>常识与世界知识</strong>：学习人类社会中普遍的事实和推理规律；</li>
<li><strong>泛化能力</strong>：具备迁移到新任务、新领域的能力。</li>
</ul>
<h3 id="1-2-为什么需要预训练？"><a href="#1-2-为什么需要预训练？" class="headerlink" title="1.2 为什么需要预训练？"></a>1.2 为什么需要预训练？</h3><h4 id="1-2-1-现实需求"><a href="#1-2-1-现实需求" class="headerlink" title="1.2.1 现实需求"></a><strong>1.2.1 现实需求</strong></h4><p>在现实场景中，很多任务都缺乏足够的标注数据。如果直接让模型从零开始在这些任务上学习，不仅效率低，而且容易过拟合。 <strong>预训练的目的</strong>就是让模型先通过大规模通用语料“自学成才”，积累通用的语言知识，再通过少量下游任务的微调，快速适应特定应用场景。</p>
<p>预训练是给模型“铺语言地基”，它需要先掌握语言的基本规律，同时也只有先具备“理解、生成人类语言”的通用能力，后续针对具体任务的调优才有意义。没有预训练，模型就没有“语言知识储备”，后续再教特定任务也学不会。</p>
<h3 id="1-3-怎么实现预训练？"><a href="#1-3-怎么实现预训练？" class="headerlink" title="1.3 怎么实现预训练？"></a>1.3 怎么实现预训练？</h3><h4 id="1-3-1-数据"><a href="#1-3-1-数据" class="headerlink" title="1.3.1 数据"></a>1.3.1 数据</h4><p>在预训练阶段，<strong>数据是模型能力的根基</strong>。训练大语言模型的第一步是收集<strong>海量且高质量的文本数据</strong>，目标是构建一个多样化、覆盖面广、内容可信的数据集，使模型能够学习丰富的语言知识与上下文关系。</p>
<h5 id="（1）数据来源与规模"><a href="#（1）数据来源与规模" class="headerlink" title="（1）数据来源与规模"></a><strong>（1）数据来源与规模</strong></h5><p> 数据通常包括：</p>
<ul>
<li><strong>网页内容</strong>（维基百科、新闻网站、论坛）</li>
<li><strong>书籍与论文</strong>（电子书、学术出版物）</li>
<li><strong>社交媒体文本</strong>（微博、Reddit、Twitter等）</li>
<li><strong>高质量对话与问答数据</strong>（如StackExchange、Quora）</li>
</ul>
<p>目前主流大语言模型的预训练语料量级通常达到 <strong>万亿级词元（Token）</strong>，例如 GPT-3 训练使用了约 5000 亿 Token，开源模型 LLaMA-2 使用约 2 万亿 Token。</p>
<h5 id="（2）数据清洗与处理流程"><a href="#（2）数据清洗与处理流程" class="headerlink" title="（2）数据清洗与处理流程"></a><strong>（2）数据清洗与处理流程</strong></h5><p> 收集的数据需经过严格的清洗与规范化流程：</p>
<ul>
<li>去除<strong>重复、无意义、低质量</strong>文本；</li>
<li>过滤<strong>色情、暴力、歧视、虚假信息</strong>等有害内容；</li>
<li>标准化文本格式，去除HTML标记、异常符号；</li>
<li>使用分词器（Tokenizer）将文本转换为<strong>词元序列</strong>；</li>
<li>将词元划分为<strong>批（Batch）</strong>输入模型训练。</li>
</ul>
<p>由于模型的语言理解与生成能力<strong>高度依赖数据质量</strong>，因此<strong>“高质量、多样性、干净”</strong>的语料是决定模型性能的关键因素。</p>
<hr>
<h4 id="1-3-2-算法"><a href="#1-3-2-算法" class="headerlink" title="1.3.2 算法"></a>1.3.2 算法</h4><h5 id="（1）核心架构：Transformer"><a href="#（1）核心架构：Transformer" class="headerlink" title="（1）核心架构：Transformer"></a>（1）核心架构：Transformer</h5><p>当前大语言模型的预训练几乎全部基于 <strong>Transformer 架构</strong>。其核心思想是<strong>自注意力机制（Self-Attention）</strong>，它能让模型在处理文本序列时，动态捕捉词语之间的全局依赖关系。例如在句子“猫追着它跑”中，模型可通过注意力机制判断“它”指代“猫”，而非仅依赖局部上下文。与传统的 RNN 或 CNN 结构相比，Transformer 具有：</p>
<ul>
<li><strong>全局建模能力强</strong>：能同时关注句中任意两个词的关系；</li>
<li><strong>并行计算高效</strong>：摆脱了序列依赖瓶颈，适合GPU/TPU加速；</li>
<li><strong>可扩展性好</strong>：层数与参数规模可线性扩展至数千亿级别。</li>
</ul>
<p>因此，Transformer 成为现代 LLM 的通用架构底座。</p>
<h5 id="（2）训练范式与任务设计"><a href="#（2）训练范式与任务设计" class="headerlink" title="（2）训练范式与任务设计"></a>（2）训练范式与任务设计</h5><p>在确定架构后，需要为模型设计合适的<strong>预训练目标任务</strong>。前主流的大模型预训练范式主要包括三类：</p>
<hr>
<p>1️⃣ Encoder-only 模型（代表：BERT）</p>
<ul>
<li><strong>核心任务：掩码语言建模（Masked Language Modeling, MLM）</strong><br>模型会随机遮盖句子中部分词语（如“猫在[MASK]上睡觉”），要求根据上下文预测被遮盖的词（“床”）。<br>这种<strong>双向上下文建模</strong>让模型更好地理解语义和语法结构。</li>
<li><strong>适用场景：</strong> 文本分类、情感分析、命名实体识别、信息抽取等理解类任务。</li>
<li><strong>技术特征：</strong><ul>
<li>强调语言理解，不具备生成能力；</li>
<li>模型结构仅包含 Transformer Encoder；</li>
<li>训练目标侧重<strong>语义表示学习</strong>。</li>
</ul>
</li>
</ul>
<hr>
<p>2️⃣ Decoder-only 模型（代表：GPT 系列、LLaMA、Baichuan）</p>
<ul>
<li><strong>核心任务：自回归语言建模（Autoregressive Language Modeling, LM）</strong><br>模型按从左到右的顺序预测下一个词，比如输入“今天我去”，预测“上学”“旅游”等最可能的下一个词。<br>通过这种方式，模型学习<strong>语言生成的连贯性与逻辑性</strong>。</li>
<li><strong>适用场景：</strong> 文本生成、对话系统、代码补全、写作辅助等。</li>
<li><strong>技术特征：</strong><ul>
<li>仅包含 Transformer Decoder；</li>
<li>强调生成能力，但理解能力相对较弱；</li>
<li>支持长文本生成与上下文建模。</li>
</ul>
</li>
</ul>
<hr>
<p>3️⃣ Encoder-Decoder 模型（代表：T5、BART、mT5）</p>
<ul>
<li><strong>核心任务：序列到序列学习（Seq2Seq）</strong><br>将输入序列（如英文句子）编码为语义表示，再解码生成目标序列（如中文翻译）。<br>通常结合掩码重建、去噪自动编码等任务训练。</li>
<li><strong>适用场景：</strong> 机器翻译、摘要生成、问答、文本改写等“输入—输出”类任务。</li>
<li><strong>技术特征：</strong><ul>
<li>同时具备理解与生成能力；</li>
<li>模型结构包含 Transformer Encoder 与 Decoder 两部分；</li>
<li>训练任务设计灵活，可统一多种NLP任务形式。</li>
</ul>
</li>
</ul>
<hr>
<p>🌟 小结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型类型</th>
<th>核心任务</th>
<th>优势</th>
<th>代表模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>Encoder-only</td>
<td>掩码语言建模（MLM）</td>
<td>强理解能力</td>
<td>BERT、RoBERTa</td>
</tr>
<tr>
<td>Decoder-only</td>
<td>自回归语言建模（LM）</td>
<td>强生成能力</td>
<td>GPT、LLaMA、Baichuan</td>
</tr>
<tr>
<td>Encoder-Decoder</td>
<td>序列到序列（Seq2Seq）</td>
<td>理解+生成兼备</td>
<td>T5、BART</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h5 id="（3）训练优化策略"><a href="#（3）训练优化策略" class="headerlink" title="（3）训练优化策略"></a>（3）训练优化策略</h5><p>在预训练中，还需使用一系列优化算法和技巧来确保模型高效收敛：</p>
<ul>
<li><strong>梯度下降（Gradient Descent）</strong>：迭代更新模型参数，使预测误差最小化；</li>
<li><strong>学习率调度（Learning Rate Scheduling）</strong>：采用 warm-up、cosine decay 等策略防止训练初期震荡或后期过拟合；</li>
<li><strong>混合精度训练（Mixed Precision）</strong>：提升显存利用率和训练速度；</li>
<li><strong>分布式并行（Data/Model/Optimizer Parallelism）</strong>：实现多GPU或多节点协同训练。</li>
</ul>
<p>这些技术共同确保了在超大规模参数和数据下的稳定、高效训练。</p>
<hr>
<h4 id="1-3-3-算力"><a href="#1-3-3-算力" class="headerlink" title="1.3.3 算力"></a>1.3.3 算力</h4><p>算力是预训练的“燃料”。由于模型规模与数据量的指数增长，预训练阶段对计算资源的需求极为庞大。</p>
<h5 id="（1）算力的定义与衡量"><a href="#（1）算力的定义与衡量" class="headerlink" title="（1）算力的定义与衡量"></a><strong>（1）算力的定义与衡量</strong></h5><p> 算力通常由以下维度共同衡量：</p>
<ul>
<li><strong>GPU/TPU数量与性能</strong>（如A100、H100数量）；</li>
<li><strong>显存容量</strong>（影响并行批大小与序列长度）；</li>
<li><strong>训练时长</strong>（通常以 GPU·小时 或 GPU·天 表示）；</li>
<li><strong>网络带宽与通信效率</strong>（影响多机分布式同步速度）。</li>
</ul>
<h5 id="（2）算力需求示例"><a href="#（2）算力需求示例" class="headerlink" title="（2）算力需求示例"></a><strong>（2）算力需求示例</strong></h5><ul>
<li>训练 <strong>百亿参数模型</strong>，通常需百卡规模GPU集群（如100×A100 80G），训练时间约需数周至数月；</li>
<li>训练 <strong>千亿参数模型</strong>（如GPT-3级），需千卡甚至万卡规模集群，训练时间可达数月。</li>
</ul>
<p>算力规模的扩展不仅关乎硬件数量，更依赖于高效的分布式训练框架（如 DeepSpeed、Megatron-LM、Colossal-AI）与集群调度系统的优化。</p>
<h2 id="二、有监督微调（SFT）"><a href="#二、有监督微调（SFT）" class="headerlink" title="二、有监督微调（SFT）"></a>二、有监督微调（SFT）</h2><h3 id="2-1-什么是SFT？"><a href="#2-1-什么是SFT？" class="headerlink" title="2.1 什么是SFT？"></a>2.1 什么是SFT？</h3><p>自2018 年以来，随着 <strong>BERT</strong> 和 <strong>GPT</strong> 等预训练模型的提出，自然语言处理（NLP）的范式从传统的“有监督学习”转向了“<strong>自监督预训练 + 有监督微调（Fine-tuning）</strong>”的新阶段。 预训练阶段让模型在大规模无标注语料中学习通用语言知识，而微调阶段则通过带标注的数据让模型适应特定任务，以实现对下游任务适配。</p>
<p><strong>SFT（Supervised Fine-Tuning）</strong> 指的是在预训练模型的基础上，利用<strong>人工标注的任务数据（输入-输出对）</strong>进行有监督训练，让模型学会执行明确的指令或任务。例如：</p>
<blockquote>
<p>若希望模型回答“如何申请年假”，就需构建大量“问题（指令）→ 标准回答（标注）”的训练样本，让模型学习从指令到回答的映射关系。</p>
</blockquote>
<p>通过 SFT，模型从“能理解语言”进一步进化为“能理解意图并执行任务”，是从 <strong>通用模型</strong> 向 <strong>专用模型</strong> 转化的关键环节。</p>
<h3 id="2-2-为什么需要SFT？"><a href="#2-2-为什么需要SFT？" class="headerlink" title="2.2 为什么需要SFT？"></a>2.2 为什么需要SFT？</h3><p>现有的类似于ChatGPT和文心一言等大模型，对于个人和小型科研团队难以获取和训练。尽管可直接使用的开源的大语言模型（如 LLaMA、Baichuan、GLM 等）已经具备强大的语言理解与生成能力，但距离特定领域应用还有一段距离<strong>，需要对其参数做进一步调整, 以提升理解用户语言和遵循用户指令的能力</strong>。</p>
<p>因此，在模型投入实际应用前，需要通过 SFT 来进行“第二阶段训练”，将通用语言能力转化为 <strong>符合特定领域、特定任务需求的能力</strong>。可以认为：</p>
<blockquote>
<p><strong>预训练教会模型“语言规律”，而 SFT 教会模型“任务执行”与“人类意图”。</strong></p>
</blockquote>
<h4 id="2-2-1-适用性-——-从“通用”到“专用”"><a href="#2-2-1-适用性-——-从“通用”到“专用”" class="headerlink" title="2.2.1 适用性 —— 从“通用”到“专用”"></a>2.2.1 适用性 —— 从“通用”到“专用”</h4><p><strong>有监督微调的首要目的，是提升模型的适用性。</strong>预训练模型掌握的是广义的语言知识和常识，但不同任务往往有独特的语境、格式和术语。例如：</p>
<ul>
<li>医疗领域包含“病历、诊断、药物相互作用”等专业词汇；</li>
<li>金融领域需要理解“资产负债率、风险敞口”等概念；</li>
<li>法律文本注重条款逻辑和判例推理。</li>
</ul>
<p>通过在这些领域的高质量标注数据上进行微调，模型能够学习到领域特有的语言表达与逻辑模式，从而提升在该领域任务上的准确性与可靠性。这使得通用大模型转变为<strong>面向场景的专用模型（Domain-Specific Model）</strong></p>
<h4 id="2-2-2-数据隐私与安全-——-从“可用”到“可信”"><a href="#2-2-2-数据隐私与安全-——-从“可用”到“可信”" class="headerlink" title="2.2.2 数据隐私与安全 —— 从“可用”到“可信”"></a>2.2.2 数据隐私与安全 —— 从“可用”到“可信”</h4><p>在实际应用中，通用大模型往往基于<strong>公开数据</strong>训练，这意味着：</p>
<ul>
<li>模型输出可能无意中包含训练语料片段；</li>
<li>在处理敏感领域（医疗、政务、金融）时存在隐私风险；</li>
<li>输出内容缺乏安全审查机制。</li>
</ul>
<p><strong>通过 SFT，可引入企业或机构自有的、符合安全规范的标注数据集进行再训练，</strong>从而在模型层面实现内容过滤与合规控制、防止隐私信息泄露。</p>
<h4 id="2-2-3-计算资源与成本-——-从“庞然大物”到“可落地”"><a href="#2-2-3-计算资源与成本-——-从“庞然大物”到“可落地”" class="headerlink" title="2.2.3 计算资源与成本 —— 从“庞然大物”到“可落地”"></a>2.2.3 计算资源与成本 —— 从“庞然大物”到“可落地”</h4><p>训练一个大模型从零开始成本极高。以 GPT-3（175B 参数）为例，单次预训练成本高达 <strong>460 万美元</strong>，需要 <strong>数千张 GPU</strong> 训练数月之久。 <strong>然而，大多数下游任务并不需要如此庞大的模型能力</strong>。<strong>SFT 提供了一种高性价比的解决方案：</strong></p>
<ul>
<li>只需加载现有的预训练模型；</li>
<li>在较小规模的特定数据上微调；</li>
<li>即可获得定制化能力。</li>
</ul>
<p>这样既能复用预训练模型的通用知识，又能避免重复投入巨大算力资源。因此，SFT 是中小型团队或企业将 LLM 技术“快速落地”的关键手段。</p>
<h3 id="2-3-怎么实现SFT？"><a href="#2-3-怎么实现SFT？" class="headerlink" title="2.3 怎么实现SFT？"></a>2.3 怎么实现SFT？</h3><p>SFT 的核心流程可概括为：</p>
<blockquote>
<p><strong>准备高质量指令数据 → 构建微调数据集 → 设计训练任务与参数 → 优化与评估</strong></p>
</blockquote>
<hr>
<h4 id="2-3-1-数据准备"><a href="#2-3-1-数据准备" class="headerlink" title="2.3.1 数据准备"></a>2.3.1 数据准备</h4><p>SFT数据集的核心是“<strong>指令–响应（instruction–response）对</strong>”，通常包含三类信息：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;请用一句话总结下这篇新闻。&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今日科技公司发布新一代AI芯片，可提升能效50%。&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;科技公司发布能效提升50%的新AI芯片。&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p><strong>（1）数据来源</strong></p>
<ul>
<li><strong>人工标注</strong>：人工编写或校验问答对，质量最高，常用于关键领域（如政务、医疗）。</li>
<li><strong>众包或专家数据</strong>：利用专业知识构建行业任务集。</li>
<li><strong>生成式合成</strong>：用已有大模型（如GPT-4）生成初始问答数据，再经人工筛选修正。</li>
<li><strong>开源数据集</strong>：如 Alpaca、Dolly等。</li>
</ul>
<p><strong>（2）数据清洗与筛选</strong></p>
<ul>
<li>去除重复、低质量或含有错误标签的数据；</li>
<li>过滤无关、敏感、低多样性的样本；</li>
<li>控制“指令类型”平衡：问答、摘要、翻译、分类、推理等；</li>
<li>对输出进行标准化（如去除口语化、统一格式）。</li>
</ul>
<p><strong>（3）数据规模</strong></p>
<p> 小型SFT通常需数万到数十万条数据；大模型（如ChatGLM3、LLaMA3）使用的数据规模往往超过<strong>百万条样本级别</strong>。</p>
<h4 id="2-3-2-指令形式与格式设计"><a href="#2-3-2-指令形式与格式设计" class="headerlink" title="2.3.2 指令形式与格式设计"></a>2.3.2 指令形式与格式设计</h4><p>SFT的关键是“<strong>指令模板化（Instruction Templating）</strong>”。即在输入中明确区分<strong>系统角色、用户指令、上下文内容</strong>，帮助模型更稳定地理解任务。</p>
<p>例如（ChatML格式）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;|system|&gt;</span><br><span class="line">你是一名专业的客服助手。</span><br><span class="line">&lt;|user|&gt;</span><br><span class="line">如何申请年假？</span><br><span class="line">&lt;|assistant|&gt;</span><br><span class="line">请假需提前三天向直属领导提交审批，审批通过后在人事系统登记。</span><br></pre></td></tr></table></figure>
<p>或者（Alpaca风格）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### Instruction:</span></span><br><span class="line">解释为什么天空是蓝色的。</span><br><span class="line"><span class="comment">### Response:</span></span><br><span class="line">因为大气中的分子对阳光中的蓝光散射更强，所以我们看到天空呈现蓝色。</span><br></pre></td></tr></table></figure>
<p>良好的模板设计能提升模型对指令结构的敏感度，从而提高响应一致性。</p>
<hr>
<h4 id="2-3-3-训练流程"><a href="#2-3-3-训练流程" class="headerlink" title="2.3.3 训练流程"></a>2.3.3 训练流程</h4><ol>
<li><strong>加载预训练模型权重</strong>（如 LLaMA、BLOOM、ChatGLM、Qwen）；</li>
<li><strong>构建数据加载器（Dataloader）</strong>；</li>
<li><strong>定义损失函数</strong>：常用交叉熵损失（CrossEntropy Loss），计算模型预测输出与标注输出的差距；</li>
<li><strong>训练过程</strong>：<ul>
<li>输入（instruction + input） → 模型；</li>
<li>模型生成预测输出；</li>
<li>对比标注答案，反向传播更新参数；</li>
</ul>
</li>
<li><strong>保存模型与检查点（checkpoint）</strong>，用于断点续训或评估。</li>
</ol>
<hr>
<h4 id="2-3-4-优化技巧与实践经验"><a href="#2-3-4-优化技巧与实践经验" class="headerlink" title="2.3.4 优化技巧与实践经验"></a>2.3.4 优化技巧与实践经验</h4><div class="table-container">
<table>
<thead>
<tr>
<th>技巧类别</th>
<th>内容与作用</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>参数高效微调（PEFT）</strong></td>
<td>如 LoRA、Prefix-Tuning、Adapter 等，仅调整少量参数即可获得良好性能，显著减少显存需求。</td>
</tr>
<tr>
<td><strong>梯度裁剪（Gradient Clipping）</strong></td>
<td>防止梯度爆炸，提高训练稳定性。</td>
</tr>
<tr>
<td><strong>学习率调度（LR Scheduler）</strong></td>
<td>常用 Cosine 或 Linear Decay 策略，使训练初期快速收敛，后期平滑稳定。</td>
</tr>
<tr>
<td><strong>混合精度训练（FP16/BF16）</strong></td>
<td>降低显存占用，提高训练速度。</td>
</tr>
<tr>
<td><strong>批量大小与梯度累积</strong></td>
<td>在显存有限时使用梯度累积（Gradient Accumulation）模拟大批量训练。</td>
</tr>
<tr>
<td><strong>Prompt平衡与多样性控制</strong></td>
<td>保证数据中任务类型分布合理，防止模型偏向单一任务。</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h4 id="2-3-5-模型评估"><a href="#2-3-5-模型评估" class="headerlink" title="2.3.5 模型评估"></a>2.3.5 模型评估</h4><p>微调完成后，需要通过<strong>指令响应测试集</strong>验证模型效果：</p>
<ul>
<li><strong>自动评估指标</strong>：BLEU、ROUGE、Accuracy、F1、Perplexity；</li>
<li><strong>人工评估维度</strong>：相关性（Relevance）、流畅性（Fluency）、事实性（Factuality）、有害性（Safety）。</li>
</ul>
<p>同时可进行<strong>A/B测试</strong>对比SFT前后模型的表现差异。</p>
<h3 id="2-4-常见微调技术："><a href="#2-4-常见微调技术：" class="headerlink" title="2.4 常见微调技术："></a>2.4 常见微调技术：</h3><p>微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。因为预训练模型参数太多，直接全量重训（Full Fine-tuning）成本过高，因此衍生出多种“高效微调”方法，目的是<strong>平衡“效果”与“成本”</strong>的关系，这里首先列举几种常见的微调方式。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>参数量</th>
<th>优点</th>
<th>缺点</th>
<th>适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full Fine-tuning</td>
<td>全参数更新</td>
<td>高精度，可完全适配新任务</td>
<td>成本高、显存需求大、易遗忘原能力</td>
<td>自建模型更新</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>只更新部分权重矩阵</td>
<td>高效、可插拔、迁移性强</td>
<td>参数选择敏感、对小数据任务仍可能过拟合</td>
<td>常见于行业微调</td>
</tr>
<tr>
<td>QLoRA</td>
<td>LoRA + 量化</td>
<td>显存占用极低，支持消费级GPU训练</td>
<td>量化可能导致精度下降、需仔细调节量化位宽</td>
<td>消费级GPU环境</td>
</tr>
<tr>
<td>Prompt Tuning</td>
<td>Prompt embedding 向量</td>
<td>不改主干参数、训练极快、参数量最小</td>
<td>对复杂任务效果有限、依赖提示设计质量</td>
<td>特定下游任务微调</td>
</tr>
<tr>
<td>Prefix Tuning</td>
<td>attention每层 KV prefix 向量</td>
<td>轻量化、快速收敛</td>
<td>学习能力有限，对长上下文任务适配性较差</td>
<td>小任务、文本生成</td>
</tr>
<tr>
<td><strong>Adapter Tuning</strong></td>
<td>增加中间层模块</td>
<td>模块化管理、支持多任务并行</td>
<td>推理时略有开销、模型结构需改动</td>
<td>多任务场景</td>
</tr>
<tr>
<td>BitFit</td>
<td>只更新Bias项</td>
<td>极低训练成本、实现简单</td>
<td>能力提升有限、对复杂任务不适用</td>
<td>实验性或对比研究</td>
</tr>
</tbody>
</table>
</div>
<h4 id="2-4-1-Full-Fine-tuning（全参数微调）"><a href="#2-4-1-Full-Fine-tuning（全参数微调）" class="headerlink" title="2.4.1 Full Fine-tuning（全参数微调）"></a>2.4.1 <strong>Full Fine-tuning（全参数微调）</strong></h4><ul>
<li>是什么：如下所示，会微调模型<strong>所有参数</strong>，让模型全方位适配任务。</li>
<li>为什么用：若算力充足，追求“效果拉满”，所有参数被调整，能最大程度达到业务所需效果。</li>
<li>怎么用：用SFT数据直接有监督训练全量参数。缺点：GPU显存需求大、训练时间长、成本高。</li>
</ul>
<p><img src="https://binaryoracle.github.io/assets/1-CSSMydgz.png" alt="图1: 反向传播更新模型参数过程"></p>
<h4 id="2-4-2-LoRA（低秩适应）"><a href="#2-4-2-LoRA（低秩适应）" class="headerlink" title="2.4.2 LoRA（低秩适应）"></a>2.4.2 <strong>LoRA（低秩适应）</strong></h4><ul>
<li>是什么：LoRA背后有一个假设，我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。<strong><em>通俗来讲：大模型参数很多，但并不是所有的参数都是发挥同样作用的；大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型</em></strong></li>
<li>为什么用：在“效果不错”的前提下，<strong>大幅降低算力/显存成本</strong>（例：全微调需10块A100，LoRA可能仅需2块）。</li>
<li>怎么用：冻结预训练模型大部分参数，仅训练部分指定的权重矩阵W，但不会直接训练更新该W矩阵，而是初始化两个低秩矩阵A和B，在准备好的数据集上，用反向传播算法更新低秩矩阵A和B的参数，使得模型在下游任务上的表现逐渐优化。在实际微调中常常对模型较深的层（而且更常调整注意力层），仅训练这些矩阵参数，训练后合并矩阵与原参数，推理速度几乎不受影响。详细理解可以参考<a target="_blank" rel="noopener" href="https://binaryoracle.github.io/LLM/应用层/LoRA微调系列.html">通俗易懂讲解LoRA微调 | MetaMind</a></li>
</ul>
<p><img src="image-20251017134429782.png" alt=""></p>
<h4 id="2-4-3-QLoRA（量化-LoRA）"><a href="#2-4-3-QLoRA（量化-LoRA）" class="headerlink" title="2.4.3 QLoRA（量化+LoRA）"></a>2.4.3 <strong>QLoRA（量化+LoRA）</strong></h4><ul>
<li>是什么：在LoRA基础上，对预训练模型做<strong>4bit量化压缩</strong>，进一步节省显存。我们知道 LoRA 的思想是：<strong>不改主干、只训练少量低秩矩阵</strong>，这大幅降低训练时可训练参数的显存占用。而QLoRA 在此基础上再把<strong>主干模型本身的权重用 4-bit 的压缩格式放到 GPU</strong>上（并保持可反向传播），因此<strong>把模型状态在显存中占用的大小降得很低</strong>。这样结果就是主干几乎不占可训练显存（因为它是“压缩+冻结”的），训练时的显存主要用于 LoRA 的小矩阵、激活、优化器状态等，从而能在一张中等显存卡上训练很大的模型。</li>
<li>为什么用：让“千亿参数超大模型”能在<strong>单块高端GPU</strong>上微调。例：千亿模型全微调跑不起来，QLoRA可在单卡A100上完成。</li>
<li>怎么用：<strong>QLoRA 不是把模型“直接丢成 4-bit 然后训练”</strong>，而是有很多工程/算法技巧保证<strong>性能与可反向传播性</strong>，比如NF4（NormalFloat 4-bit）数据类型、Double Quantization（双重量化）、Paged Optimizers（分页优化器 / 内存调度）等等，这里不在赘述，可以参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/666234324">QLoRA（Quantized LoRA）详解</a>。Qlora最明显的优点就是显存暴降；但同时也会有缺点：量化损失少量精度，但对效果影响极小。</li>
</ul>
<p><img src="image-20251017145123036.png" alt="Qlora和lora等对比"></p>
<h4 id="2-4-4-Prompt-Tuning（提示微调）"><a href="#2-4-4-Prompt-Tuning（提示微调）" class="headerlink" title="2.4.4 Prompt Tuning（提示微调）"></a>2.4.4 <strong>Prompt Tuning（提示微调）</strong></h4><ul>
<li>是什么：不怎么动模型参数，仅训练“提示（Prompt）”参数，给任务加一段特殊“前缀提示”，让模型通过学习前缀适配任务。Prompt Tuning是发生在Embedding这个环节的。<strong><em>如果将大模型比做一个函数：Y=f(X)，那么Prompt Tuning就是在保证函数本身不变的前提下，在X前面加上了一些特定的内容，而这些内容可以影响X生成期望中Y的概率。</em></strong> 从数学的角度来分析：Prompt Tuning 的核心是<strong>训练 “可学习的嵌入向量（Soft Prompt）” 作为输入前缀</strong>，而非直接拼接文本提示。这些嵌入向量与模型的词嵌入维度一致（如 768 维），会被拼接在原始输入的嵌入层之前，引导模型适配任务。例如，训练一组形如 <code>[P1, P2, P3, P4]</code> 的可学习嵌入（每个 P 是一个向量），拼接在 “这部电影很有趣。总体评价是 [MASK] 的” 之前，让模型学习用这些嵌入向量捕捉任务模式。它本质是<strong>监督微调的一种 “参数高效变体”</strong>—— 训练数据与普通 SFT 一致（“指令 - 标注输出” 对），但仅更新 “提示嵌入” 的参数（通常仅占总参数的 0.1%~1%），模型主体完全冻结。</li>
<li>为什么用：Prompt Tuning的出发点，是基座模型(Foundation Model)的参数不变，<strong>为每个特定任务，训练一个少量参数的小模型，在具体执行特定任务的时候按需调用</strong>，由于<strong>参数更新量极小</strong>这个特点，适合“多任务快速切换”（电商问答—&gt;医疗问答，换Prompt即可）。</li>
<li>怎么用：首先<strong>构造输入</strong>，将可学习的 “提示嵌入” 与原始输入的嵌入层拼接。例如，输入文本 “特效很棒，但剧情糟糕。” 的嵌入会与训练好的 <code>[P1, P2, P3, P4]</code> 嵌入拼接，形成完整输入。冻结预训练模型所有参数，仅更新 “提示嵌入” 的参数。用 SFT 数据（“指令 - 标注输出” 对）计算损失（如掩码预测的交叉熵），反向传播时仅优化提示嵌入。训练完成后，同一任务的所有查询都使用这套 “提示嵌入”。缺点：复杂任务效果明显弱于LoRA/全微调</li>
</ul>
<p><img src="image-20251017145410883.png" alt="Prompt Tuning"></p>
<h4 id="2-4-5-Prefix-Tuning（前缀微调）"><a href="#2-4-5-Prefix-Tuning（前缀微调）" class="headerlink" title="2.4.5 Prefix Tuning（前缀微调）"></a>2.4.5 <strong>Prefix Tuning（前缀微调）</strong></h4><ul>
<li>是什么：出发点类似Prompt Tuning，但具体实现上与前者在Embedding环节往输入序列X前面加特定的Token不同，Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。训练“连续前缀向量”（不是文本，而是设计的模型能理解的向量），对输入影响更精细。</li>
<li>为什么用：比Prompt Tuning更灵活，效果略好，且仍保持“参数效率高”。</li>
<li>怎么用：在模型注意力q，v等权重向量前加“可训练前缀向量”，用SFT数据训练向量，模型主体不动。适合想提升Prompt Tuning效果、又不想多训参数的场景。</li>
</ul>
<p><img src="image-20251017150244409.png" alt="image-20251017150244409"></p>
<h4 id="2-4-6-Adapter-Tuning（适配器微调）"><a href="#2-4-6-Adapter-Tuning（适配器微调）" class="headerlink" title="2.4.6 Adapter Tuning（适配器微调）"></a>2.4.6 <strong>Adapter Tuning（适配器微调）</strong></h4><ul>
<li>是什么：在预训练模型各层之间插入“小型适配器网络”（如几个全连接层），仅训练适配器。</li>
<li>为什么用：<strong>对主模型干扰小</strong>（主参数冻结），同时适配器能针对性学任务特征；推理时仅增加少量延迟。</li>
<li>怎么用：在模型层（如Transformer编码器层）间插入Adapter，用SFT数据训练Adapter参数。适合既要保主模型通用能力、又要适配任务的场景。</li>
</ul>
<p><img src="image-20251017151020760.png" alt="image-20251017151020760"></p>
<h4 id="2-4-7-BitFit（偏置微调）"><a href="#2-4-7-BitFit（偏置微调）" class="headerlink" title="2.4.7 BitFit（偏置微调）"></a>2.4.7 <strong>BitFit（偏置微调）</strong></h4><ul>
<li>是什么：仅训练模型的“偏置（Bias）”参数（可理解为“辅助调节开关”），其他参数全冻结。</li>
<li>为什么用：<strong>参数更新量极小</strong>，训练超快，适合“快速验证任务可行性”。</li>
<li>怎么用：仅开放模型中所有Bias参数的训练权限，用SFT数据训练。缺点：复杂任务效果一般，适合简单任务/前期试错。</li>
</ul>
<h2 id="三、对齐训练（Alignment-Training）"><a href="#三、对齐训练（Alignment-Training）" class="headerlink" title="三、对齐训练（Alignment Training）"></a>三、对齐训练（Alignment Training）</h2><h3 id="3-1-什么是对齐训练？"><a href="#3-1-什么是对齐训练？" class="headerlink" title="3.1 什么是对齐训练？"></a>3.1 什么是对齐训练？</h3><p>有监督微调（SFT）让模型“会做事”，但它并不保证模型“做得对”。<strong>对齐训练（Alignment Training）</strong> 的目标是进一步让模型的输出<strong>符合人类的价值观、审美与行为准则</strong>，确保它在各种复杂场景中既“有能力”又“有边界”。</p>
<p>简单来说，对齐是让模型从：</p>
<blockquote>
<p>“我知道答案” → “我知道人类希望我怎么回答”。</p>
</blockquote>
<p>因此，Alignment 是大语言模型迈向可信与可控智能的关键环节。它的核心追求是构建一个<strong>有益（Helpful）</strong>、<strong>诚实（Honest）</strong>、<strong>无害（Harmless）</strong>的模型，也被称为 “3H 原则”。</p>
<hr>
<h3 id="3-2-为什么要对齐？"><a href="#3-2-为什么要对齐？" class="headerlink" title="3.2 为什么要对齐？"></a>3.2 为什么要对齐？</h3><p>预训练与SFT只能让模型学习语言规律与任务执行，但无法保证：</p>
<ul>
<li><strong>输出是否符合伦理与价值观</strong>（如不得生成攻击性言论）；</li>
<li><strong>回答是否符合事实</strong>（防止“自信地胡说”）；</li>
<li><strong>行为是否符合用户意图</strong>（避免曲解、答非所问）；</li>
<li><strong>是否具备稳定的风格与安全边界</strong>。</li>
</ul>
<p>如果缺乏对齐训练，模型可能出现以下问题：</p>
<ul>
<li>生成有害或偏见性内容（toxicity, bias）；</li>
<li>出现“幻觉”（hallucination）；</li>
<li>不服从人类指令（non-compliance）；</li>
<li>行为不一致，导致用户信任度下降。</li>
</ul>
<p>因此，对齐训练是连接 <strong>技术能力</strong> 与 <strong>人类价值</strong> 的关键桥梁，使模型输出更贴近人类偏好和社会规范。</p>
<hr>
<h3 id="3-3-对齐的主要实现方式"><a href="#3-3-对齐的主要实现方式" class="headerlink" title="3.3 对齐的主要实现方式"></a>3.3 对齐的主要实现方式</h3><p>目前主流的对齐训练方式主要包括三种范式：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>对齐范式</th>
<th>核心思想</th>
<th>优点</th>
<th>代表模型</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>基于人类反馈的强化学习（RLHF）</strong></td>
<td>通过奖励模型学习人类偏好，用强化学习优化LLM行为</td>
<td>效果成熟、业界标准</td>
<td>ChatGPT、Claude、PaLM</td>
</tr>
<tr>
<td><strong>直接偏好优化（DPO）</strong></td>
<td>直接将人类偏好数据转化为损失函数微调LLM，无需训练奖励模型</td>
<td>训练简单、稳定性高</td>
<td>LLaMA3、Mistral、Yi系列</td>
</tr>
<tr>
<td><strong>基于AI反馈的强化学习（RLAIF）</strong></td>
<td>由强大的教师模型替代人类提供反馈，实现自动化对齐</td>
<td>成本低、可规模化</td>
<td>Gemini、GPT-4-turbo 系列</td>
</tr>
</tbody>
</table>
</div>
<p>下面重点介绍前两种核心范式：RLHF 和 DPO。</p>
<hr>
<h4 id="3-3-1-RLHF（Reinforcement-Learning-from-Human-Feedback）"><a href="#3-3-1-RLHF（Reinforcement-Learning-from-Human-Feedback）" class="headerlink" title="3.3.1 RLHF（Reinforcement Learning from Human Feedback）"></a>3.3.1 RLHF（Reinforcement Learning from Human Feedback）</h4><h5 id="（1）核心思想"><a href="#（1）核心思想" class="headerlink" title="（1）核心思想"></a>（1）核心思想</h5><p>RLHF 目标是让模型输出更符合人类偏好，<strong>通过“人类打分”来引导模型行为的强化学习闭环</strong>。其典型流程分为三步：</p>
<ol>
<li><p><strong>Supervised Fine-Tuning（SFT）</strong><br>在预训练模型上使用人工标注的高质量指令数据进行监督微调，使模型具备初步的指令理解能力。</p>
</li>
<li><p><strong>Reward Model（RM）训练</strong></p>
<ul>
<li>收集模型对<strong>同一指令生成的多个回答</strong>；</li>
<li>由<strong>人类标注者比较这些回答的好坏</strong>（如选出“更符合人类偏好”的答案）；</li>
<li>用这些对比数据训练一个<strong>奖励模型</strong> ( $R_\phi$ )，学会预测人类更倾向哪个输出。</li>
</ul>
<p>换言之，RM 学的是人类“偏好函数”。</p>
</li>
<li><p><strong>强化学习优化（如PPO算法）</strong></p>
<ul>
<li>使用 RM 的评分作为奖励信号；</li>
<li>以语言模型 ( $\pi_\theta$ ) 为智能体，优化其生成策略，使高分回答概率更大；</li>
<li>常采用 <strong>PPO（Proximal Policy Optimization）</strong> 进行训练，以防止模型输出分布漂移过大导致“崩坏”。</li>
</ul>
</li>
</ol>
<p>整个流程如下：</p>
<blockquote>
<p><strong>SFT → 奖励模型（RM） → 强化学习优化（PPO）</strong></p>
</blockquote>
<p>示意图如下：</p>
<p><img src="https://syhya.github.io/zh/posts/2025-02-08-dpo/InstructGPT.png" alt="img"></p>
<h5 id="（2）奖励模型的关键要素"><a href="#（2）奖励模型的关键要素" class="headerlink" title="（2）奖励模型的关键要素"></a>（2）奖励模型的关键要素</h5><ul>
<li>输入：prompt + candidate responses；</li>
<li>输出：一个实数分数 ( $R_\phi(x, y) $)，代表该回答被人类偏好的程度；</li>
<li>损失函数：通常采用 pairwise ranking loss，使 ($ R_\phi(y_{preferred}) &gt; R_\phi(y_{rejected})$ )。</li>
</ul>
<h5 id="（3）PPO强化学习阶段"><a href="#（3）PPO强化学习阶段" class="headerlink" title="（3）PPO强化学习阶段"></a>（3）PPO强化学习阶段</h5><ul>
<li><p>优化目标：</p>
<script type="math/tex; mode=display">
 [
 \max_\theta \mathbb{E}*{x,y\sim \pi*\theta} [R_\phi(x, y) - \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{SFT}}(y|x)}]
 ]</script><p>其中第二项为KL正则项，用于约束新模型不偏离SFT模型。</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/1853524/1759986420522-4a0bca67-c6ce-4a93-8881-24a7015db601.png" alt="RLHF流程图"></p>
</li>
</ul>
<h5 id="（4）优缺点"><a href="#（4）优缺点" class="headerlink" title="（4）优缺点"></a>（4）优缺点</h5><div class="table-container">
<table>
<thead>
<tr>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>输出质量高、行为自然、偏好一致性好</td>
<td>成本高（需大量人工反馈）、训练复杂（RM+PPO）、稳定性较难控制</td>
</tr>
</tbody>
</table>
</div>
<p>尽管 RLHF 能充分利用人类偏好信息提升模型对齐效果，但其固有局限性包括：</p>
<ul>
<li><strong>多模型训练</strong>：除 Actor 模型外，还需额外训练奖励模型$R_\phi$和 Critic 模型，整体训练过程复杂且资源消耗大。</li>
<li><strong>高采样成本</strong>：LLM 生成文本计算量大，强化学习过程中的大量在线采样进一步推高了计算开销；采样不足可能导致错误的优化方向。</li>
<li><strong>训练不稳定与超参数敏感</strong>：PPO 涉及众多超参数（如学习率、采样量等），调参复杂且训练过程易受不稳定因素影响。</li>
<li><strong>对齐税效应</strong>：在提高模型对齐性的同时，可能会降低模型在其他任务上的表现。</li>
</ul>
<h3 id="3-3-2-DPO（Direct-Preference-Optimization）"><a href="#3-3-2-DPO（Direct-Preference-Optimization）" class="headerlink" title="3.3.2 DPO（Direct Preference Optimization）"></a>3.3.2 DPO（Direct Preference Optimization）</h3><p>RLHF 虽然效果好，但<strong>流程复杂，成本高昂并且训练不稳定，对超参数敏感</strong>。为此，研究者提出了更简化的方案 —— <strong>DPO（直接偏好优化）</strong>。</p>
<h5 id="（1）核心思想-1"><a href="#（1）核心思想-1" class="headerlink" title="（1）核心思想"></a>（1）核心思想</h5><p>DPO <strong>不再训练独立的奖励模型，也不使用强化学习算法</strong>，而是<strong>直接利用偏好对比数据定义一个新的损失函数来优化语言模型</strong>，其核心思路是将 RLHF 的目标转化为类似于监督微调的对比学习任务</p>
<p>训练数据形式仍是「偏好对」：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(prompt, chosen_response, rejected_response)</span><br></pre></td></tr></table></figure>
<p>即人类认为 “chosen” 比 “rejected” 更好。</p>
<p>DPO 通过一个基于对数几率的目标函数，让模型倾向于生成“chosen”样本，而避免“rejected”样本，公式如下：</p>
<script type="math/tex; mode=display">
[
 L_{\text{DPO}}(\theta) = -\log \sigma\Big(\beta \cdot [\log \pi_\theta(y_c|x) - \log \pi_\theta(y_r|x) - \log \pi_{\text{ref}}(y_c|x) + \log \pi_{\text{ref}}(y_r|x)]\Big)
 ]</script><p>其中：</p>
<ul>
<li>( $\pi_\theta$ )：当前待优化模型；</li>
<li>( $\pi_{\text{ref}} $)：SFT后的参考模型；</li>
<li>( $\beta $)：温度系数，调控对齐强度；</li>
<li>( $y_c$, $y_r$ )：分别为“优选”与“弃选”回答。</li>
</ul>
<p><img src="https://syhya.github.io/zh/posts/2025-02-08-dpo/rlhf_dpo.png" alt="img"></p>
<h5 id="（2）优点"><a href="#（2）优点" class="headerlink" title="（2）优点"></a>（2）优点</h5><ul>
<li>无需训练独立RM，<strong>直接微调LLM本体</strong>；</li>
<li>训练稳定、收敛快；</li>
<li>可无缝结合LoRA等轻量化方案；</li>
<li>效果接近甚至优于RLHF。</li>
</ul>
<h5 id="（3）变体：LoRA-DPO"><a href="#（3）变体：LoRA-DPO" class="headerlink" title="（3）变体：LoRA-DPO"></a>（3）变体：LoRA-DPO</h5><p>在显存有限的情况下，可结合 <strong>LoRA（Low-Rank Adaptation）</strong> 技术，仅对少量参数进行低秩更新，实现：</p>
<blockquote>
<p>“轻量参数 + 高效对齐” 的平衡。<br>LoRA-DPO 已成为开源模型（如 LLaMA3、Yi-1.6、Qwen1.5）的主流对齐方式。</p>
</blockquote>
<hr>
<h3 id="3-4-小结"><a href="#3-4-小结" class="headerlink" title="3.4 小结"></a>3.4 小结</h3><div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>目标</th>
<th>代表方法</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SFT</strong></td>
<td>让模型学会执行任务</td>
<td>有监督微调</td>
<td>教“会不会做”</td>
</tr>
<tr>
<td><strong>RLHF</strong></td>
<td>让模型更符合人类偏好</td>
<td>RM + PPO</td>
<td>教“做得对不对”</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>用更简洁的方式实现对齐</td>
<td>直接偏好优化</td>
<td>教“做得更好、更稳定”</td>
</tr>
</tbody>
</table>
</div>
<h2 id="四、场景应用与实战指南"><a href="#四、场景应用与实战指南" class="headerlink" title="四、场景应用与实战指南"></a>四、场景应用与实战指南</h2><p>前文我们已经系统地了解了预训练、SFT和对齐训练的理论知识。但在实际项目中，我们面临的第一个问题往往是：“我的任务到底需不需要微调？需要哪种微调？” 本节将聚焦于这些实际问题，提供场景化的决策参考和一个简化的实战流程。</p>
<h3 id="4-1-SFT-场景辨析：何时必须做？"><a href="#4-1-SFT-场景辨析：何时必须做？" class="headerlink" title="4.1 SFT 场景辨析：何时必须做？"></a>4.1 SFT 场景辨析：何时必须做？</h3><p><strong>SFT（有监督微调）并非总是必须的。</strong> 对于一些简单的、通用的任务，强大的基础模型（如 GPT-4, Llama 3）通过精心设计的 <strong>Prompt Engineering</strong> 或 <strong>少样本学习（Few-shot Learning）</strong> 就可能获得足够好的效果，这样做成本最低、速度最快。</p>
<p><strong>那么，什么场景下 SFT 是强烈推荐甚至必须的呢？</strong></p>
<h4 id="场景一：领域知识注入-Domain-Adaptation"><a href="#场景一：领域知识注入-Domain-Adaptation" class="headerlink" title="场景一：领域知识注入-Domain Adaptation"></a><strong>场景一：领域知识注入-Domain Adaptation</strong></h4><p>当你的任务需要模型掌握特定领域的专业知识、术语或内部“黑话”时，SFT是必选项。</p>
<ul>
<li><p><strong>问题</strong>：通用模型不了解你公司的产品、内部流程或特定行业的专业术语。</p>
</li>
<li><p>例子：</p>
<ul>
<li><strong>医疗领域</strong>：让模型能看懂病历，并根据《XX疾病诊断指南》生成摘要。通用模型可能不认识某些药物名称或诊断缩写。</li>
<li><strong>金融领域</strong>：让模型担任投研助手，它必须理解“风险敞口”、“阿尔法收益”等专业词汇，并能解析特定公司的财报。</li>
<li><strong>企业内部</strong>：让模型成为新员工的入职助手，需要回答“如何申请‘星辰计划’项目经费？”这类公司内部问题。</li>
</ul>
</li>
<li><p><strong>SFT 作用</strong>：通过包含这些专业知识的指令数据进行微调，相当于给模型“补课”，让它成为该领域的“专家”。</p>
</li>
</ul>
<h4 id="场景二：特定任务格式遵循-Task-Format-Adherence"><a href="#场景二：特定任务格式遵循-Task-Format-Adherence" class="headerlink" title="场景二：特定任务格式遵循-Task Format Adherence"></a><strong>场景二：特定任务格式遵循-Task Format Adherence</strong></h4><p>当你的任务要求模型严格按照某种特定格式输出时，SFT的效果远胜于复杂的Prompt。</p>
<ul>
<li><p><strong>问题</strong>：Prompt有时不稳定，模型可能“自由发挥”，无法保证每次都输出你想要的格式（如 JSON、Markdown 表格、XML等）。</p>
</li>
<li><p>例子：</p>
<ul>
<li><strong>信息抽取</strong>：从一段非结构化文本中，抽取出人名、公司和职位，并以固定的 <strong>JSON 格式</strong> 返回。</li>
<li><strong>代码生成</strong>：要求模型根据需求，生成符合公司内部编码规范（如特定的变量命名、注释风格）的代码。</li>
<li><strong>客服摘要</strong>：将一段长对话自动总结为包含“客户诉求”、“解决方案”、“待办事项”三个字段的报告。</li>
</ul>
</li>
<li><p><strong>SFT 作用</strong>：通过大量格式正确的“输入-输出”对进行训练，让模型形成“肌肉记忆”，从而稳定、可靠地生成所需格式。</p>
</li>
</ul>
<h4 id="场景三：特定风格-人设模仿-Style-Persona-Imitation"><a href="#场景三：特定风格-人设模仿-Style-Persona-Imitation" class="headerlink" title="场景三：特定风格/人设模仿-Style/Persona Imitation"></a><strong>场景三：特定风格/人设模仿-Style/Persona Imitation</strong></h4><p>当需要模型以一种独特的语气、风格或人设进行交流时，SFT是塑造其“性格”的最佳方式。</p>
<ul>
<li><p><strong>问题</strong>：通用模型的回应风格通常是中立、客观的“AI助手”风格，不符合品牌或产品定位。</p>
</li>
<li><p>例子：</p>
<ul>
<li><strong>儿童教育产品</strong>：需要模型扮演一个活泼、可爱、有耐心的“故事大王”角色。</li>
<li><strong>法律咨询助手</strong>：需要模型使用严谨、正式、专业的法律术语进行回复。</li>
<li><strong>品牌营销文案</strong>：需要模型模仿特定品牌（如“杜蕾斯”）的犀利、幽默风格。</li>
</ul>
</li>
<li><p><strong>SFT 作用</strong>：SFT 数据集中的 <code>output</code> 不仅是正确答案，也承载了期望的风格。模型在学习答案的同时，也学会了“如何说”。</p>
</li>
</ul>
<blockquote>
<p>🎯 <strong>决策小结</strong>：</p>
<ul>
<li>如果你的任务用 <strong>Prompt</strong> 就能稳定解决，<strong>那就别做 SFT</strong>。</li>
<li>如果涉及<strong>专业知识、固定格式、特定人设</strong>，且 Prompt 效果不稳定或成本太高，<strong>SFT 就是你的最佳选择</strong>。</li>
</ul>
</blockquote>
<h3 id="4-2-指令数据的多样性：从单轮到多轮与CoT"><a href="#4-2-指令数据的多样性：从单轮到多轮与CoT" class="headerlink" title="4.2 指令数据的多样性：从单轮到多轮与CoT"></a>4.2 指令数据的多样性：从单轮到多轮与CoT</h3><p>准确的来说监督微调，可以包含指令微调（明确任务指令，如翻译）对话微调（多轮对话数据，如客服）领域适配（特定领域术语，如医疗）⽂本分类（结构化标签，如情感分析）模型推理微调（思维链标注，如数学解题）</p>
<p>除了前文提到的 <code>instruction-input-output</code> 结构，指令对的形式也需要根据任务需求进行设计，以激发模型不同的能力。</p>
<h4 id="1-单轮指令（Single-Turn-Instruction）"><a href="#1-单轮指令（Single-Turn-Instruction）" class="headerlink" title="1. 单轮指令（Single-Turn Instruction）"></a><strong>1. 单轮指令（Single-Turn Instruction）</strong></h4><p>最基础的形式，适用于简单的问答、分类、摘要等一次性完成的任务。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;将下面这句话翻译成英文。&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;今天天气真不错。&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;The weather is really nice today.&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="2-多轮对话（Multi-Turn-Dialogue）"><a href="#2-多轮对话（Multi-Turn-Dialogue）" class="headerlink" title="2. 多轮对话（Multi-Turn Dialogue）"></a><strong>2. 多轮对话（Multi-Turn Dialogue）</strong></h4><p>用于构建能理解上下文的对话机器人。数据集需要包含完整的对话历史，让模型学会联系上下文进行回复。</p>
<ul>
<li><strong>格式</strong>：通常采用类似 <code>ChatML</code> 的格式，用角色（system, user, assistant）来区分对话内容。</li>
<li><strong>例子</strong>：</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;human&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好，帮我推荐一款适合新手的笔记本电脑，预算5000元左右。&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;好的，这个预算内，您可以考虑联想小新Pro 14或者小米笔记本Pro 15。您更看重轻薄便携还是屏幕素质呢？&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;human&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我经常出差，所以轻薄便携更重要。&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;from&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gpt&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;明白了。那么联想小新Pro 14会更适合您，它的重量只有1.3kg，非常便于携带。性能也完全满足日常办公和影音娱乐。&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>训练目标</strong>：模型在生成第二个<code>&quot;gpt&quot;</code>回答时，必须同时考虑两个<code>&quot;human&quot;</code>的输入（预算和便携性要求）。</li>
</ul>
<h4 id="3-思维链（Chain-of-Thought-CoT）"><a href="#3-思维链（Chain-of-Thought-CoT）" class="headerlink" title="3. 思维链（Chain of Thought, CoT）"></a><strong>3. 思维链（Chain of Thought, CoT）</strong></h4><p>用于提升模型在<strong>逻辑推理、数学计算、复杂规划</strong>等任务上的能力。CoT 的核心是不仅告诉模型“答案是什么”，更要教它“如何一步步得到答案”。</p>
<ul>
<li><strong>格式</strong>：在 <code>output</code> 中，先包含详细的思考过程，再给出最终答案。</li>
<li><strong>例子</strong>：</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;农场里有10只鸡和8只兔子，请问它们一共有多少条腿？&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;好的，我们来一步步计算：\n1. 首先计算鸡的腿数：鸡有2条腿，所以10只鸡一共有 10 * 2 = 20 条腿。\n2. 接着计算兔子的腿数：兔子有4条腿，所以8只兔子一共有 8 * 4 = 32 条腿。\n3. 最后将它们的腿数相加：20 + 32 = 52 条腿。\n所以，它们一共有52条腿。&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>训练目标</strong>：通过学习 CoT 数据，模型能够学会将复杂问题分解为简单步骤，并按逻辑顺序求解，从而大幅提高推理任务的准确率。</li>
</ul>
<h3 id="4-3-对齐训练场景辨析：DPO-RLHF-是必需品吗？"><a href="#4-3-对齐训练场景辨析：DPO-RLHF-是必需品吗？" class="headerlink" title="4.3 对齐训练场景辨析：DPO/RLHF 是必需品吗？"></a>4.3 对齐训练场景辨析：DPO/RLHF 是必需品吗？</h3><p>与SFT类似，<strong>对齐训练（如DPO）也并非总是必须的</strong>。</p>
<h4 id="场景一：对齐训练是强需求的"><a href="#场景一：对齐训练是强需求的" class="headerlink" title="场景一：对齐训练是强需求的"></a><strong>场景一：对齐训练是强需求的</strong></h4><p>当你的模型需要直接面向大量、不可控的公众用户时，对齐几乎是<strong>必备环节</strong>。</p>
<ul>
<li><p><strong>问题</strong>：SFT 后的模型虽然“能干活”，<strong>但可能“乱说话”</strong>。它可能生成<strong>有害、偏见、不安全或虚假的内容</strong>，引发严重的品牌或法律风险。</p>
</li>
<li><p>例子：</p>
<ul>
<li><strong>公共聊天机器人</strong>（如文心一言、ChatGPT）：必须确保对敏感问题（政治、宗教、暴力等）有安全、无害的回复。</li>
<li><strong>AI 心理咨询助手</strong>：必须避免给出有害或不负责任的建议，学会共情和提供积极引导。</li>
<li><strong>高风险决策辅助</strong>（如医疗诊断建议）：模型需要诚实地表达不确定性（“我无法确定，请咨询专业医生”），而不是“自信地胡说八道”（幻觉）。</li>
</ul>
</li>
<li><p><strong>DPO/RLHF 作用</strong>：通过人类偏好数据，为模型建立一套“价值观”，教会它在复杂的道德和安全边界问题上做出“正确”的选择。<strong>DPO 更像是在给模型划定“红线”和“底线”</strong>。</p>
</li>
</ul>
<h4 id="场景二：对齐训练需求较弱或非必需的"><a href="#场景二：对齐训练需求较弱或非必需的" class="headerlink" title="场景二：对齐训练需求较弱或非必需的"></a><strong>场景二：对齐训练需求较弱或非必需的</strong></h4><ul>
<li><strong>内部工具或后端服务</strong>：如果模型仅用于公司内部，由专业人士使用（如程序员用它生成代码、研究员用它分析数据），用户有能力辨别输出的质量，那么对齐的优先级就较低。</li>
<li><strong>高度受控的特定任务</strong>：如果模型只执行一个非常狭窄的任务（如从财报中提取营收数字），其输入和输出都高度可控，产生有害内容的风险极低。</li>
<li><strong>SFT 数据质量极高</strong>：如果你的 SFT 数据本身就已经蕴含了强烈的偏好信号（即所有 <code>output</code> 都是非常优质、安全、负责的回答），那么在某种程度上，SFT 本身也起到了“隐式对齐”的作用。</li>
</ul>
<blockquote>
<p>🎯 <strong>决策小结</strong>：</p>
<ul>
<li><strong>面向公众、开放域、高风险</strong>的应用，<strong>必须做对齐</strong>。</li>
<li><strong>内部使用、窄领域、低风险</strong>的应用，可以<strong>优先考虑不做或延后做</strong>，以节约成本。</li>
</ul>
</blockquote>
<h3 id="4-4-案例：从通用模型到垂直领域专家"><a href="#4-4-案例：从通用模型到垂直领域专家" class="headerlink" title="4.4 案例：从通用模型到垂直领域专家"></a>4.4 案例：从通用模型到垂直领域专家</h3><p>让我们通过一个<strong>金融客服助手</strong>的例子，串联起整个流程。</p>
<ol>
<li><p><strong>阶段一：选择基础模型（预训练）</strong></p>
<ul>
<li><strong>目标</strong>：需要一个懂中文、有基本对话能力的模型。</li>
<li><strong>选择</strong>：<code>Llama-3-8B-Instruct</code> 或 <code>Qwen1.5-7B-Chat</code>。这些模型已经完成了预训练，具备了强大的通用语言能力。</li>
<li><strong>现状</strong>：此时的模型像一个“什么都懂一点，但什么都不精”的通才。你问它“什么是市盈率？”，它能答对；但你问“我们公司的‘金卡理财’产品年化收益率是多少？”，它会说“我不知道”。</li>
</ul>
</li>
<li><p><strong>阶段二：领域知识与能力微调（SFT）</strong></p>
<ul>
<li><p><strong>目标</strong>：让模型成为一名合格的“XX银行”金融产品客服。</p>
</li>
<li><p>数据准备：</p>
<ul>
<li><strong>知识型</strong>：<code>&quot;instruction&quot;: &quot;介绍一下我们的‘稳健增长’基金&quot;</code>, <code>&quot;output&quot;: &quot;‘稳健增长’基金是...&quot;</code></li>
<li><strong>任务型（多轮）</strong>：构建用户咨询、查询余额、办理业务的模拟对话。</li>
<li><strong>风格型</strong>：所有 <code>output</code> 都采用礼貌、专业、严谨的客服话术。</li>
</ul>
</li>
<li><p><strong>训练</strong>：使用 <strong>QLoRA</strong> 在 SFT 数据集上进行微调。</p>
</li>
<li><p><strong>结果</strong>：得到一个 <code>Finance-LLM-SFT</code> 模型。它现在能准确回答公司产品问题，并以客服的口吻交流。但当你问它“我应该把所有钱都投到股票里吗？”，它可能会给出不负责任的建议。</p>
</li>
</ul>
</li>
<li><p><strong>阶段三：安全与价值观对齐（DPO）</strong></p>
<ul>
<li><p><strong>目标</strong>：确保模型不提供投资建议、不泄露隐私、回答安全可靠。</p>
</li>
<li><p>数据准备（偏好对）：</p>
<ul>
<li><strong>Prompt</strong>: “我应该把所有钱都投到股票里吗？”</li>
<li><strong>Chosen (更优)</strong>: “作为AI助手，我无法提供投资建议。投资有风险，建议您咨询专业的理财顾问。”</li>
<li><strong>Rejected (更差)</strong>: “是的，股市回报高，值得一试！”</li>
</ul>
</li>
<li><p><strong>训练</strong>：在 <code>Finance-LLM-SFT</code> 模型基础上，用偏好数据集进行 <strong>DPO</strong> 训练。</p>
</li>
<li><p><strong>结果</strong>：得到最终的 <code>Finance-LLM-SFT-DPO</code> 模型。它不仅是业务专家，更是一个<strong>安全、可靠、值得信赖</strong>的金融助手。</p>
</li>
</ul>
</li>
</ol>
<h3 id="4-5-微调实战入门指南（以-QLoRA-SFT-为例）"><a href="#4-5-微调实战入门指南（以-QLoRA-SFT-为例）" class="headerlink" title="4.5 微调实战入门指南（以 QLoRA SFT 为例）"></a>4.5 微调实战入门指南（以 QLoRA SFT 为例）</h3><p>这里提供一个使用 Hugging Face 生态（<code>transformers</code>, <code>peft</code>, <code>trl</code>）进行 QLoRA 微调的简化步骤，让你对整个流程有体感。</p>
<p><strong>第一步：环境准备</strong><br>安装必要的库。<code>bitsandbytes</code> 是量化所必需的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch transformers datasets peft accelerate bitsandbytes trl</span><br></pre></td></tr></table></figure>
<p><strong>第二步：选择基础模型和分词器</strong><br>选择一个合适的开源模型，比如 Qwen1.5。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">&quot;Qwen/Qwen1.5-7B-Chat&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置4-bit量化</span></span><br><span class="line">quantization_config = BitsAndBytesConfig(</span><br><span class="line">    load_in_4bit=<span class="literal">True</span>,</span><br><span class="line">    bnb_4bit_compute_dtype=torch.bfloat16,</span><br><span class="line">    bnb_4bit_quant_type=<span class="string">&quot;nf4&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载量化后的模型和分词器</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_name,</span><br><span class="line">    quantization_config=quantization_config,</span><br><span class="line">    device_map=<span class="string">&quot;auto&quot;</span> <span class="comment"># 自动分配到GPU</span></span><br><span class="line">)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line">tokenizer.pad_token = tokenizer.eos_token <span class="comment"># 设置pad_token</span></span><br></pre></td></tr></table></figure>
<p><strong>第三步：准备和处理数据集</strong><br>假设你有一个 JSONL 格式的数据集 <code>my_dataset.jsonl</code>，每行是一个对话。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;messages&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;system&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;You are a helpful assistant.&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;你好！有什么可以帮助你的吗？&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;messages&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;user&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;请用一句话总结下这篇新闻。&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span> <span class="punctuation">&#123;</span><span class="attr">&quot;role&quot;</span><span class="punctuation">:</span> <span class="string">&quot;assistant&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="string">&quot;科技公司发布能效提升50%的新AI芯片。&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">]</span><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>使用 <code>datasets</code> 库加载，并应用聊天模板。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;json&quot;</span>, data_files=<span class="string">&quot;my_dataset.jsonl&quot;</span>, split=<span class="string">&quot;train&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个处理函数，将对话格式化为模型输入</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">formatting_prompts_func</span>(<span class="params">example</span>):</span><br><span class="line">    output_texts = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(example[<span class="string">&#x27;messages&#x27;</span>])):</span><br><span class="line">        text = tokenizer.apply_chat_template(example[<span class="string">&#x27;messages&#x27;</span>][i], tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">False</span>)</span><br><span class="line">        output_texts.append(text)</span><br><span class="line">    <span class="keyword">return</span> output_texts</span><br></pre></td></tr></table></figure>
<p><strong>第四步：配置 LoRA 和训练参数</strong><br>使用 <code>peft</code> 配置 LoRA，使用 <code>transformers</code> 配置训练参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line"></span><br><span class="line"><span class="comment"># LoRA 配置</span></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,  <span class="comment"># 低秩矩阵的秩</span></span><br><span class="line">    lora_alpha=<span class="number">16</span>,</span><br><span class="line">    target_modules=[<span class="string">&quot;q_proj&quot;</span>, <span class="string">&quot;k_proj&quot;</span>, <span class="string">&quot;v_proj&quot;</span>, <span class="string">&quot;o_proj&quot;</span>], <span class="comment"># 对注意力模块应用LoRA</span></span><br><span class="line">    lora_dropout=<span class="number">0.05</span>,</span><br><span class="line">    bias=<span class="string">&quot;none&quot;</span>,</span><br><span class="line">    task_type=<span class="string">&quot;CAUSAL_LM&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用PEFT</span></span><br><span class="line">model = get_peft_model(model, lora_config)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练参数</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./qwen-7b-sft&quot;</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    gradient_accumulation_steps=<span class="number">4</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-4</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    logging_steps=<span class="number">10</span>,</span><br><span class="line">    save_steps=<span class="number">100</span>,</span><br><span class="line">    fp16=<span class="literal">True</span>, <span class="comment"># 使用混合精度训练</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><strong>第五步：开始训练</strong><br>使用 <code>trl</code> 库的 <code>SFTTrainer</code>，它极大地简化了指令微调的流程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> trl <span class="keyword">import</span> SFTTrainer</span><br><span class="line"></span><br><span class="line">trainer = SFTTrainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=dataset,</span><br><span class="line">    packing=<span class="literal">True</span>, <span class="comment"># 将多个短样本打包成一个长样本，提高效率</span></span><br><span class="line">    formatting_func=formatting_prompts_func, <span class="comment"># 使用我们定义的数据处理函数</span></span><br><span class="line">    max_seq_length=<span class="number">1024</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">trainer.train()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存LoRA适配器</span></span><br><span class="line">trainer.save_model(<span class="string">&quot;./qwen-7b-sft-lora&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>第六步：推理测试</strong><br>加载基础模型和训练好的 LoRA 适配器进行测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> PeftModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载基础模型</span></span><br><span class="line">base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="string">&quot;auto&quot;</span>)</span><br><span class="line"><span class="comment"># 加载LoRA权重</span></span><br><span class="line">model = PeftModel.from_pretrained(base_model, <span class="string">&quot;./qwen-7b-sft-lora&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并模型（可选，合并后推理更快，但无法再卸载LoRA）</span></span><br><span class="line"><span class="comment"># model = model.merge_and_unload()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">prompt = <span class="string">&quot;请用一句话总结下这篇新闻。&quot;</span></span><br><span class="line">messages = [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: prompt&#125;]</span><br><span class="line">text = tokenizer.apply_chat_template(messages, tokenize=<span class="literal">False</span>, add_generation_prompt=<span class="literal">True</span>)</span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">&quot;pt&quot;</span>).to(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">outputs = model.generate(**inputs, max_new_tokens=<span class="number">50</span>)</span><br><span class="line"><span class="built_in">print</span>(tokenizer.decode(outputs[<span class="number">0</span>], skip_special_tokens=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure>
<p>这个实战指南只是一个起点和示例，目前是用代码的方式来实现，当然还有零代码就能进行微调的LLamA-Factory，实际项目中还需要关注<strong>数据清洗、超参调优、模型评估</strong>等更多细节。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">5ha</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/">http://example.com/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">5ha的个人空间</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></div><div class="post-share"><div class="social-share" data-image="/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/image-20251017134429782.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/11/17/myfeelings/" title="myfeelings"><img class="cover" src="/2025/11/17/myfeelings/SMU.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">myfeelings</div></div><div class="info-2"><div class="info-item-1">阳光明媚  </div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">5ha</div><div class="author-info-description">春风得意马蹄疾，一日看尽长安花</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">2</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/worlds3"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/worlds3" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:vmerluckycat5@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">2025.11.17 完成个人博客的搭建，欢迎来到我的个人博客 — 更新科研/学习/生活笔记。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%BE%AE%E8%B0%83"><span class="toc-text">大模型训练与微调</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-text">一、预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-text">1.1 什么是预训练？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-%E8%83%8C%E6%99%AF%E4%B8%8E%E6%A6%82%E5%BF%B5"><span class="toc-text">1.1.1 背景与概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2-%E7%B1%BB%E6%AF%94%E7%90%86%E8%A7%A3"><span class="toc-text">1.1.2 类比理解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-3-%E6%A0%B8%E5%BF%83%E7%9B%AE%E6%A0%87"><span class="toc-text">1.1.3 核心目标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-text">1.2 为什么需要预训练？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1-%E7%8E%B0%E5%AE%9E%E9%9C%80%E6%B1%82"><span class="toc-text">1.2.1 现实需求</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-text">1.3 怎么实现预训练？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1-%E6%95%B0%E6%8D%AE"><span class="toc-text">1.3.1 数据</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90%E4%B8%8E%E8%A7%84%E6%A8%A1"><span class="toc-text">（1）数据来源与规模</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-text">（2）数据清洗与处理流程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2-%E7%AE%97%E6%B3%95"><span class="toc-text">1.3.2 算法</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84%EF%BC%9ATransformer"><span class="toc-text">（1）核心架构：Transformer</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E8%AE%AD%E7%BB%83%E8%8C%83%E5%BC%8F%E4%B8%8E%E4%BB%BB%E5%8A%A1%E8%AE%BE%E8%AE%A1"><span class="toc-text">（2）训练范式与任务设计</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-text">（3）训练优化策略</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-3-%E7%AE%97%E5%8A%9B"><span class="toc-text">1.3.3 算力</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E7%AE%97%E5%8A%9B%E7%9A%84%E5%AE%9A%E4%B9%89%E4%B8%8E%E8%A1%A1%E9%87%8F"><span class="toc-text">（1）算力的定义与衡量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E7%AE%97%E5%8A%9B%E9%9C%80%E6%B1%82%E7%A4%BA%E4%BE%8B"><span class="toc-text">（2）算力需求示例</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%EF%BC%88SFT%EF%BC%89"><span class="toc-text">二、有监督微调（SFT）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E4%BB%80%E4%B9%88%E6%98%AFSFT%EF%BC%9F"><span class="toc-text">2.1 什么是SFT？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81SFT%EF%BC%9F"><span class="toc-text">2.2 为什么需要SFT？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-%E9%80%82%E7%94%A8%E6%80%A7-%E2%80%94%E2%80%94-%E4%BB%8E%E2%80%9C%E9%80%9A%E7%94%A8%E2%80%9D%E5%88%B0%E2%80%9C%E4%B8%93%E7%94%A8%E2%80%9D"><span class="toc-text">2.2.1 适用性 —— 从“通用”到“专用”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-%E6%95%B0%E6%8D%AE%E9%9A%90%E7%A7%81%E4%B8%8E%E5%AE%89%E5%85%A8-%E2%80%94%E2%80%94-%E4%BB%8E%E2%80%9C%E5%8F%AF%E7%94%A8%E2%80%9D%E5%88%B0%E2%80%9C%E5%8F%AF%E4%BF%A1%E2%80%9D"><span class="toc-text">2.2.2 数据隐私与安全 —— 从“可用”到“可信”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%E4%B8%8E%E6%88%90%E6%9C%AC-%E2%80%94%E2%80%94-%E4%BB%8E%E2%80%9C%E5%BA%9E%E7%84%B6%E5%A4%A7%E7%89%A9%E2%80%9D%E5%88%B0%E2%80%9C%E5%8F%AF%E8%90%BD%E5%9C%B0%E2%80%9D"><span class="toc-text">2.2.3 计算资源与成本 —— 从“庞然大物”到“可落地”</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0SFT%EF%BC%9F"><span class="toc-text">2.3 怎么实现SFT？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-text">2.3.1 数据准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-%E6%8C%87%E4%BB%A4%E5%BD%A2%E5%BC%8F%E4%B8%8E%E6%A0%BC%E5%BC%8F%E8%AE%BE%E8%AE%A1"><span class="toc-text">2.3.2 指令形式与格式设计</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-text">2.3.3 训练流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-4-%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7%E4%B8%8E%E5%AE%9E%E8%B7%B5%E7%BB%8F%E9%AA%8C"><span class="toc-text">2.3.4 优化技巧与实践经验</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-text">2.3.5 模型评估</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%B8%B8%E8%A7%81%E5%BE%AE%E8%B0%83%E6%8A%80%E6%9C%AF%EF%BC%9A"><span class="toc-text">2.4 常见微调技术：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-1-Full-Fine-tuning%EF%BC%88%E5%85%A8%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83%EF%BC%89"><span class="toc-text">2.4.1 Full Fine-tuning（全参数微调）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-2-LoRA%EF%BC%88%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94%EF%BC%89"><span class="toc-text">2.4.2 LoRA（低秩适应）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-QLoRA%EF%BC%88%E9%87%8F%E5%8C%96-LoRA%EF%BC%89"><span class="toc-text">2.4.3 QLoRA（量化+LoRA）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-4-Prompt-Tuning%EF%BC%88%E6%8F%90%E7%A4%BA%E5%BE%AE%E8%B0%83%EF%BC%89"><span class="toc-text">2.4.4 Prompt Tuning（提示微调）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-5-Prefix-Tuning%EF%BC%88%E5%89%8D%E7%BC%80%E5%BE%AE%E8%B0%83%EF%BC%89"><span class="toc-text">2.4.5 Prefix Tuning（前缀微调）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-6-Adapter-Tuning%EF%BC%88%E9%80%82%E9%85%8D%E5%99%A8%E5%BE%AE%E8%B0%83%EF%BC%89"><span class="toc-text">2.4.6 Adapter Tuning（适配器微调）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-7-BitFit%EF%BC%88%E5%81%8F%E7%BD%AE%E5%BE%AE%E8%B0%83%EF%BC%89"><span class="toc-text">2.4.7 BitFit（偏置微调）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AF%B9%E9%BD%90%E8%AE%AD%E7%BB%83%EF%BC%88Alignment-Training%EF%BC%89"><span class="toc-text">三、对齐训练（Alignment Training）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%BB%80%E4%B9%88%E6%98%AF%E5%AF%B9%E9%BD%90%E8%AE%AD%E7%BB%83%EF%BC%9F"><span class="toc-text">3.1 什么是对齐训练？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%AF%B9%E9%BD%90%EF%BC%9F"><span class="toc-text">3.2 为什么要对齐？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%AF%B9%E9%BD%90%E7%9A%84%E4%B8%BB%E8%A6%81%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F"><span class="toc-text">3.3 对齐的主要实现方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-RLHF%EF%BC%88Reinforcement-Learning-from-Human-Feedback%EF%BC%89"><span class="toc-text">3.3.1 RLHF（Reinforcement Learning from Human Feedback）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">（1）核心思想</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%85%B3%E9%94%AE%E8%A6%81%E7%B4%A0"><span class="toc-text">（2）奖励模型的关键要素</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89PPO%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%98%B6%E6%AE%B5"><span class="toc-text">（3）PPO强化学习阶段</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-text">（4）优缺点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-DPO%EF%BC%88Direct-Preference-Optimization%EF%BC%89"><span class="toc-text">3.3.2 DPO（Direct Preference Optimization）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-1"><span class="toc-text">（1）核心思想</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BC%98%E7%82%B9"><span class="toc-text">（2）优点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%8F%98%E4%BD%93%EF%BC%9ALoRA-DPO"><span class="toc-text">（3）变体：LoRA-DPO</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%B0%8F%E7%BB%93"><span class="toc-text">3.4 小结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%9C%BA%E6%99%AF%E5%BA%94%E7%94%A8%E4%B8%8E%E5%AE%9E%E6%88%98%E6%8C%87%E5%8D%97"><span class="toc-text">四、场景应用与实战指南</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-SFT-%E5%9C%BA%E6%99%AF%E8%BE%A8%E6%9E%90%EF%BC%9A%E4%BD%95%E6%97%B6%E5%BF%85%E9%A1%BB%E5%81%9A%EF%BC%9F"><span class="toc-text">4.1 SFT 场景辨析：何时必须做？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%B8%80%EF%BC%9A%E9%A2%86%E5%9F%9F%E7%9F%A5%E8%AF%86%E6%B3%A8%E5%85%A5-Domain-Adaptation"><span class="toc-text">场景一：领域知识注入-Domain Adaptation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%BA%8C%EF%BC%9A%E7%89%B9%E5%AE%9A%E4%BB%BB%E5%8A%A1%E6%A0%BC%E5%BC%8F%E9%81%B5%E5%BE%AA-Task-Format-Adherence"><span class="toc-text">场景二：特定任务格式遵循-Task Format Adherence</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%B8%89%EF%BC%9A%E7%89%B9%E5%AE%9A%E9%A3%8E%E6%A0%BC-%E4%BA%BA%E8%AE%BE%E6%A8%A1%E4%BB%BF-Style-Persona-Imitation"><span class="toc-text">场景三：特定风格&#x2F;人设模仿-Style&#x2F;Persona Imitation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%9A%E6%A0%B7%E6%80%A7%EF%BC%9A%E4%BB%8E%E5%8D%95%E8%BD%AE%E5%88%B0%E5%A4%9A%E8%BD%AE%E4%B8%8ECoT"><span class="toc-text">4.2 指令数据的多样性：从单轮到多轮与CoT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8D%95%E8%BD%AE%E6%8C%87%E4%BB%A4%EF%BC%88Single-Turn-Instruction%EF%BC%89"><span class="toc-text">1. 单轮指令（Single-Turn Instruction）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%EF%BC%88Multi-Turn-Dialogue%EF%BC%89"><span class="toc-text">2. 多轮对话（Multi-Turn Dialogue）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%80%9D%E7%BB%B4%E9%93%BE%EF%BC%88Chain-of-Thought-CoT%EF%BC%89"><span class="toc-text">3. 思维链（Chain of Thought, CoT）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%AF%B9%E9%BD%90%E8%AE%AD%E7%BB%83%E5%9C%BA%E6%99%AF%E8%BE%A8%E6%9E%90%EF%BC%9ADPO-RLHF-%E6%98%AF%E5%BF%85%E9%9C%80%E5%93%81%E5%90%97%EF%BC%9F"><span class="toc-text">4.3 对齐训练场景辨析：DPO&#x2F;RLHF 是必需品吗？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%B8%80%EF%BC%9A%E5%AF%B9%E9%BD%90%E8%AE%AD%E7%BB%83%E6%98%AF%E5%BC%BA%E9%9C%80%E6%B1%82%E7%9A%84"><span class="toc-text">场景一：对齐训练是强需求的</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%BA%8C%EF%BC%9A%E5%AF%B9%E9%BD%90%E8%AE%AD%E7%BB%83%E9%9C%80%E6%B1%82%E8%BE%83%E5%BC%B1%E6%88%96%E9%9D%9E%E5%BF%85%E9%9C%80%E7%9A%84"><span class="toc-text">场景二：对齐训练需求较弱或非必需的</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%A1%88%E4%BE%8B%EF%BC%9A%E4%BB%8E%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E5%88%B0%E5%9E%82%E7%9B%B4%E9%A2%86%E5%9F%9F%E4%B8%93%E5%AE%B6"><span class="toc-text">4.4 案例：从通用模型到垂直领域专家</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%EF%BC%88%E4%BB%A5-QLoRA-SFT-%E4%B8%BA%E4%BE%8B%EF%BC%89"><span class="toc-text">4.5 微调实战入门指南（以 QLoRA SFT 为例）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/" title="llm微调调研"><img src="/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/image-20251017134429782.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="llm微调调研"/></a><div class="content"><a class="title" href="/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/" title="llm微调调研">llm微调调研</a><time datetime="2025-11-17T16:13:40.000Z" title="发表于 2025-11-18 00:13:40">2025-11-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/11/17/myfeelings/" title="myfeelings"><img src="/2025/11/17/myfeelings/SMU.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="myfeelings"/></a><div class="content"><a class="title" href="/2025/11/17/myfeelings/" title="myfeelings">myfeelings</a><time datetime="2025-11-17T15:37:51.000Z" title="发表于 2025-11-17 23:37:51">2025-11-17</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/image-20251017134429782.png);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By 5ha</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.2</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script async data-pjax src="/"></script></div></body></html>