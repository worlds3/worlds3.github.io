[{"title":"myfeelings","url":"/2025/11/17/myfeelings/","content":"阳光明媚\n\n","categories":["生活随记"]},{"title":"llm微调调研","url":"/2025/11/18/llm%E5%BE%AE%E8%B0%83%E8%B0%83%E7%A0%94/","content":"大模型训练与微调大语言模型（LLM）的成长过程，大体可以分为三个阶段：\n\n\n\n\n阶段\n目标\n学习方式\n类比\n\n\n\n\n预训练（Pretraining）\n学习语言规律和知识\n自监督学习\n小孩学习基础语法\n\n\n监督微调（SFT）\n学会听懂“人类指令”\n人类示范样本\n老师手把手教你回答问题\n\n\n对齐训练（RLHF / DPO）\n学会“说得合适”\n人类偏好反馈\n学会在社交中说话得体\n\n\n\n\n\n🧩 简单来理解：预训练让模型“有知识”；微调让模型“能沟通”；对齐让模型“合人意”。\n\n一、预训练1.1 什么是预训练？1.1.1 背景与概念 在大语言模型（LLM, Large Language Model）中，预训练（Pre-training） 是整个模型训练流程的第一阶段，也是最关键的一步。它的目标是让模型在大规模、无标注的文本数据上，通过自监督学习（Self-supervised Learning）的方式，掌握语言的基础规律和世界常识。 这些数据通常来源于互联网网页、新闻、百科、书籍、社交媒体文本、论文等，从而使模型具备理解自然语言、生成自然语言的通用能力。\n1.1.2 类比理解 可以把预训练后的模型比作刚刚毕业的大学生——他们已经通过大学阶段的“通识教育”，掌握了丰富的理论知识与基本技能，具备了在社会中工作的基础能力。但要真正胜任某个具体岗位（比如程序员、医生、律师），还需要进行“上岗培训”和“岗位实战”，也就是模型在预训练之后所需要的微调（Fine-tuning）和对齐（Alignment）阶段。\n\n换句话说，预训练是让模型“学会语言”，而微调和对齐则是让它“懂人话”、“会办事”。\n\n1.1.3 核心目标 通过预训练，模型能够在以下方面获得能力：\n\n语言规律：掌握语法、句法结构、上下文逻辑；\n语义理解：理解词语和句子的深层含义；\n常识与世界知识：学习人类社会中普遍的事实和推理规律；\n泛化能力：具备迁移到新任务、新领域的能力。\n\n1.2 为什么需要预训练？1.2.1 现实需求在现实场景中，很多任务都缺乏足够的标注数据。如果直接让模型从零开始在这些任务上学习，不仅效率低，而且容易过拟合。 预训练的目的就是让模型先通过大规模通用语料“自学成才”，积累通用的语言知识，再通过少量下游任务的微调，快速适应特定应用场景。\n预训练是给模型“铺语言地基”，它需要先掌握语言的基本规律，同时也只有先具备“理解、生成人类语言”的通用能力，后续针对具体任务的调优才有意义。没有预训练，模型就没有“语言知识储备”，后续再教特定任务也学不会。\n1.3 怎么实现预训练？1.3.1 数据在预训练阶段，数据是模型能力的根基。训练大语言模型的第一步是收集海量且高质量的文本数据，目标是构建一个多样化、覆盖面广、内容可信的数据集，使模型能够学习丰富的语言知识与上下文关系。\n（1）数据来源与规模 数据通常包括：\n\n网页内容（维基百科、新闻网站、论坛）\n书籍与论文（电子书、学术出版物）\n社交媒体文本（微博、Reddit、Twitter等）\n高质量对话与问答数据（如StackExchange、Quora）\n\n目前主流大语言模型的预训练语料量级通常达到 万亿级词元（Token），例如 GPT-3 训练使用了约 5000 亿 Token，开源模型 LLaMA-2 使用约 2 万亿 Token。\n（2）数据清洗与处理流程 收集的数据需经过严格的清洗与规范化流程：\n\n去除重复、无意义、低质量文本；\n过滤色情、暴力、歧视、虚假信息等有害内容；\n标准化文本格式，去除HTML标记、异常符号；\n使用分词器（Tokenizer）将文本转换为词元序列；\n将词元划分为批（Batch）输入模型训练。\n\n由于模型的语言理解与生成能力高度依赖数据质量，因此“高质量、多样性、干净”的语料是决定模型性能的关键因素。\n\n1.3.2 算法（1）核心架构：Transformer当前大语言模型的预训练几乎全部基于 Transformer 架构。其核心思想是自注意力机制（Self-Attention），它能让模型在处理文本序列时，动态捕捉词语之间的全局依赖关系。例如在句子“猫追着它跑”中，模型可通过注意力机制判断“它”指代“猫”，而非仅依赖局部上下文。与传统的 RNN 或 CNN 结构相比，Transformer 具有：\n\n全局建模能力强：能同时关注句中任意两个词的关系；\n并行计算高效：摆脱了序列依赖瓶颈，适合GPU/TPU加速；\n可扩展性好：层数与参数规模可线性扩展至数千亿级别。\n\n因此，Transformer 成为现代 LLM 的通用架构底座。\n（2）训练范式与任务设计在确定架构后，需要为模型设计合适的预训练目标任务。前主流的大模型预训练范式主要包括三类：\n\n1️⃣ Encoder-only 模型（代表：BERT）\n\n核心任务：掩码语言建模（Masked Language Modeling, MLM）模型会随机遮盖句子中部分词语（如“猫在[MASK]上睡觉”），要求根据上下文预测被遮盖的词（“床”）。这种双向上下文建模让模型更好地理解语义和语法结构。\n适用场景： 文本分类、情感分析、命名实体识别、信息抽取等理解类任务。\n技术特征：\n强调语言理解，不具备生成能力；\n模型结构仅包含 Transformer Encoder；\n训练目标侧重语义表示学习。\n\n\n\n\n2️⃣ Decoder-only 模型（代表：GPT 系列、LLaMA、Baichuan）\n\n核心任务：自回归语言建模（Autoregressive Language Modeling, LM）模型按从左到右的顺序预测下一个词，比如输入“今天我去”，预测“上学”“旅游”等最可能的下一个词。通过这种方式，模型学习语言生成的连贯性与逻辑性。\n适用场景： 文本生成、对话系统、代码补全、写作辅助等。\n技术特征：\n仅包含 Transformer Decoder；\n强调生成能力，但理解能力相对较弱；\n支持长文本生成与上下文建模。\n\n\n\n\n3️⃣ Encoder-Decoder 模型（代表：T5、BART、mT5）\n\n核心任务：序列到序列学习（Seq2Seq）将输入序列（如英文句子）编码为语义表示，再解码生成目标序列（如中文翻译）。通常结合掩码重建、去噪自动编码等任务训练。\n适用场景： 机器翻译、摘要生成、问答、文本改写等“输入—输出”类任务。\n技术特征：\n同时具备理解与生成能力；\n模型结构包含 Transformer Encoder 与 Decoder 两部分；\n训练任务设计灵活，可统一多种NLP任务形式。\n\n\n\n\n🌟 小结\n\n\n\n\n模型类型\n核心任务\n优势\n代表模型\n\n\n\n\nEncoder-only\n掩码语言建模（MLM）\n强理解能力\nBERT、RoBERTa\n\n\nDecoder-only\n自回归语言建模（LM）\n强生成能力\nGPT、LLaMA、Baichuan\n\n\nEncoder-Decoder\n序列到序列（Seq2Seq）\n理解+生成兼备\nT5、BART\n\n\n\n\n\n（3）训练优化策略在预训练中，还需使用一系列优化算法和技巧来确保模型高效收敛：\n\n梯度下降（Gradient Descent）：迭代更新模型参数，使预测误差最小化；\n学习率调度（Learning Rate Scheduling）：采用 warm-up、cosine decay 等策略防止训练初期震荡或后期过拟合；\n混合精度训练（Mixed Precision）：提升显存利用率和训练速度；\n分布式并行（Data/Model/Optimizer Parallelism）：实现多GPU或多节点协同训练。\n\n这些技术共同确保了在超大规模参数和数据下的稳定、高效训练。\n\n1.3.3 算力算力是预训练的“燃料”。由于模型规模与数据量的指数增长，预训练阶段对计算资源的需求极为庞大。\n（1）算力的定义与衡量 算力通常由以下维度共同衡量：\n\nGPU/TPU数量与性能（如A100、H100数量）；\n显存容量（影响并行批大小与序列长度）；\n训练时长（通常以 GPU·小时 或 GPU·天 表示）；\n网络带宽与通信效率（影响多机分布式同步速度）。\n\n（2）算力需求示例\n训练 百亿参数模型，通常需百卡规模GPU集群（如100×A100 80G），训练时间约需数周至数月；\n训练 千亿参数模型（如GPT-3级），需千卡甚至万卡规模集群，训练时间可达数月。\n\n算力规模的扩展不仅关乎硬件数量，更依赖于高效的分布式训练框架（如 DeepSpeed、Megatron-LM、Colossal-AI）与集群调度系统的优化。\n二、有监督微调（SFT）2.1 什么是SFT？自2018 年以来，随着 BERT 和 GPT 等预训练模型的提出，自然语言处理（NLP）的范式从传统的“有监督学习”转向了“自监督预训练 + 有监督微调（Fine-tuning）”的新阶段。 预训练阶段让模型在大规模无标注语料中学习通用语言知识，而微调阶段则通过带标注的数据让模型适应特定任务，以实现对下游任务适配。\nSFT（Supervised Fine-Tuning） 指的是在预训练模型的基础上，利用人工标注的任务数据（输入-输出对）进行有监督训练，让模型学会执行明确的指令或任务。例如：\n\n若希望模型回答“如何申请年假”，就需构建大量“问题（指令）→ 标准回答（标注）”的训练样本，让模型学习从指令到回答的映射关系。\n\n通过 SFT，模型从“能理解语言”进一步进化为“能理解意图并执行任务”，是从 通用模型 向 专用模型 转化的关键环节。\n2.2 为什么需要SFT？现有的类似于ChatGPT和文心一言等大模型，对于个人和小型科研团队难以获取和训练。尽管可直接使用的开源的大语言模型（如 LLaMA、Baichuan、GLM 等）已经具备强大的语言理解与生成能力，但距离特定领域应用还有一段距离，需要对其参数做进一步调整, 以提升理解用户语言和遵循用户指令的能力。\n因此，在模型投入实际应用前，需要通过 SFT 来进行“第二阶段训练”，将通用语言能力转化为 符合特定领域、特定任务需求的能力。可以认为：\n\n预训练教会模型“语言规律”，而 SFT 教会模型“任务执行”与“人类意图”。\n\n2.2.1 适用性 —— 从“通用”到“专用”有监督微调的首要目的，是提升模型的适用性。预训练模型掌握的是广义的语言知识和常识，但不同任务往往有独特的语境、格式和术语。例如：\n\n医疗领域包含“病历、诊断、药物相互作用”等专业词汇；\n金融领域需要理解“资产负债率、风险敞口”等概念；\n法律文本注重条款逻辑和判例推理。\n\n通过在这些领域的高质量标注数据上进行微调，模型能够学习到领域特有的语言表达与逻辑模式，从而提升在该领域任务上的准确性与可靠性。这使得通用大模型转变为面向场景的专用模型（Domain-Specific Model）\n2.2.2 数据隐私与安全 —— 从“可用”到“可信”在实际应用中，通用大模型往往基于公开数据训练，这意味着：\n\n模型输出可能无意中包含训练语料片段；\n在处理敏感领域（医疗、政务、金融）时存在隐私风险；\n输出内容缺乏安全审查机制。\n\n通过 SFT，可引入企业或机构自有的、符合安全规范的标注数据集进行再训练，从而在模型层面实现内容过滤与合规控制、防止隐私信息泄露。\n2.2.3 计算资源与成本 —— 从“庞然大物”到“可落地”训练一个大模型从零开始成本极高。以 GPT-3（175B 参数）为例，单次预训练成本高达 460 万美元，需要 数千张 GPU 训练数月之久。 然而，大多数下游任务并不需要如此庞大的模型能力。SFT 提供了一种高性价比的解决方案：\n\n只需加载现有的预训练模型；\n在较小规模的特定数据上微调；\n即可获得定制化能力。\n\n这样既能复用预训练模型的通用知识，又能避免重复投入巨大算力资源。因此，SFT 是中小型团队或企业将 LLM 技术“快速落地”的关键手段。\n2.3 怎么实现SFT？SFT 的核心流程可概括为：\n\n准备高质量指令数据 → 构建微调数据集 → 设计训练任务与参数 → 优化与评估\n\n\n2.3.1 数据准备SFT数据集的核心是“指令–响应（instruction–response）对”，通常包含三类信息：\n&#123;  &quot;instruction&quot;: &quot;请用一句话总结下这篇新闻。&quot;,  &quot;input&quot;: &quot;今日科技公司发布新一代AI芯片，可提升能效50%。&quot;,  &quot;output&quot;: &quot;科技公司发布能效提升50%的新AI芯片。&quot;&#125;\n（1）数据来源\n\n人工标注：人工编写或校验问答对，质量最高，常用于关键领域（如政务、医疗）。\n众包或专家数据：利用专业知识构建行业任务集。\n生成式合成：用已有大模型（如GPT-4）生成初始问答数据，再经人工筛选修正。\n开源数据集：如 Alpaca、Dolly等。\n\n（2）数据清洗与筛选\n\n去除重复、低质量或含有错误标签的数据；\n过滤无关、敏感、低多样性的样本；\n控制“指令类型”平衡：问答、摘要、翻译、分类、推理等；\n对输出进行标准化（如去除口语化、统一格式）。\n\n（3）数据规模\n 小型SFT通常需数万到数十万条数据；大模型（如ChatGLM3、LLaMA3）使用的数据规模往往超过百万条样本级别。\n2.3.2 指令形式与格式设计SFT的关键是“指令模板化（Instruction Templating）”。即在输入中明确区分系统角色、用户指令、上下文内容，帮助模型更稳定地理解任务。\n例如（ChatML格式）：\n&lt;|system|&gt;你是一名专业的客服助手。&lt;|user|&gt;如何申请年假？&lt;|assistant|&gt;请假需提前三天向直属领导提交审批，审批通过后在人事系统登记。\n或者（Alpaca风格）：\n### Instruction:解释为什么天空是蓝色的。### Response:因为大气中的分子对阳光中的蓝光散射更强，所以我们看到天空呈现蓝色。\n良好的模板设计能提升模型对指令结构的敏感度，从而提高响应一致性。\n\n2.3.3 训练流程\n加载预训练模型权重（如 LLaMA、BLOOM、ChatGLM、Qwen）；\n构建数据加载器（Dataloader）；\n定义损失函数：常用交叉熵损失（CrossEntropy Loss），计算模型预测输出与标注输出的差距；\n训练过程：\n输入（instruction + input） → 模型；\n模型生成预测输出；\n对比标注答案，反向传播更新参数；\n\n\n保存模型与检查点（checkpoint），用于断点续训或评估。\n\n\n2.3.4 优化技巧与实践经验\n\n\n\n技巧类别\n内容与作用\n\n\n\n\n参数高效微调（PEFT）\n如 LoRA、Prefix-Tuning、Adapter 等，仅调整少量参数即可获得良好性能，显著减少显存需求。\n\n\n梯度裁剪（Gradient Clipping）\n防止梯度爆炸，提高训练稳定性。\n\n\n学习率调度（LR Scheduler）\n常用 Cosine 或 Linear Decay 策略，使训练初期快速收敛，后期平滑稳定。\n\n\n混合精度训练（FP16/BF16）\n降低显存占用，提高训练速度。\n\n\n批量大小与梯度累积\n在显存有限时使用梯度累积（Gradient Accumulation）模拟大批量训练。\n\n\nPrompt平衡与多样性控制\n保证数据中任务类型分布合理，防止模型偏向单一任务。\n\n\n\n\n\n2.3.5 模型评估微调完成后，需要通过指令响应测试集验证模型效果：\n\n自动评估指标：BLEU、ROUGE、Accuracy、F1、Perplexity；\n人工评估维度：相关性（Relevance）、流畅性（Fluency）、事实性（Factuality）、有害性（Safety）。\n\n同时可进行A/B测试对比SFT前后模型的表现差异。\n2.4 常见微调技术：微调的最终目的，是能够在可控成本的前提下，尽可能地提升大模型在特定领域的能力。因为预训练模型参数太多，直接全量重训（Full Fine-tuning）成本过高，因此衍生出多种“高效微调”方法，目的是平衡“效果”与“成本”的关系，这里首先列举几种常见的微调方式。\n\n\n\n\n方法\n参数量\n优点\n缺点\n适用场景\n\n\n\n\nFull Fine-tuning\n全参数更新\n高精度，可完全适配新任务\n成本高、显存需求大、易遗忘原能力\n自建模型更新\n\n\nLoRA\n只更新部分权重矩阵\n高效、可插拔、迁移性强\n参数选择敏感、对小数据任务仍可能过拟合\n常见于行业微调\n\n\nQLoRA\nLoRA + 量化\n显存占用极低，支持消费级GPU训练\n量化可能导致精度下降、需仔细调节量化位宽\n消费级GPU环境\n\n\nPrompt Tuning\nPrompt embedding 向量\n不改主干参数、训练极快、参数量最小\n对复杂任务效果有限、依赖提示设计质量\n特定下游任务微调\n\n\nPrefix Tuning\nattention每层 KV prefix 向量\n轻量化、快速收敛\n学习能力有限，对长上下文任务适配性较差\n小任务、文本生成\n\n\nAdapter Tuning\n增加中间层模块\n模块化管理、支持多任务并行\n推理时略有开销、模型结构需改动\n多任务场景\n\n\nBitFit\n只更新Bias项\n极低训练成本、实现简单\n能力提升有限、对复杂任务不适用\n实验性或对比研究\n\n\n\n\n2.4.1 Full Fine-tuning（全参数微调）\n是什么：如下所示，会微调模型所有参数，让模型全方位适配任务。\n为什么用：若算力充足，追求“效果拉满”，所有参数被调整，能最大程度达到业务所需效果。\n怎么用：用SFT数据直接有监督训练全量参数。缺点：GPU显存需求大、训练时间长、成本高。\n\n\n2.4.2 LoRA（低秩适应）\n是什么：LoRA背后有一个假设，我们现在看到的这些大语言模型，它们都是被过度参数化的。而过度参数化的大模型背后，都有一个低维的本质模型。通俗来讲：大模型参数很多，但并不是所有的参数都是发挥同样作用的；大模型中有其中一部分参数，是非常重要的，是影响大模型生成结果的关键参数，这部分关键参数就是上面提到的低维的本质模型\n为什么用：在“效果不错”的前提下，大幅降低算力/显存成本（例：全微调需10块A100，LoRA可能仅需2块）。\n怎么用：冻结预训练模型大部分参数，仅训练部分指定的权重矩阵W，但不会直接训练更新该W矩阵，而是初始化两个低秩矩阵A和B，在准备好的数据集上，用反向传播算法更新低秩矩阵A和B的参数，使得模型在下游任务上的表现逐渐优化。在实际微调中常常对模型较深的层（而且更常调整注意力层），仅训练这些矩阵参数，训练后合并矩阵与原参数，推理速度几乎不受影响。详细理解可以参考通俗易懂讲解LoRA微调 | MetaMind\n\n\n2.4.3 QLoRA（量化+LoRA）\n是什么：在LoRA基础上，对预训练模型做4bit量化压缩，进一步节省显存。我们知道 LoRA 的思想是：不改主干、只训练少量低秩矩阵，这大幅降低训练时可训练参数的显存占用。而QLoRA 在此基础上再把主干模型本身的权重用 4-bit 的压缩格式放到 GPU上（并保持可反向传播），因此把模型状态在显存中占用的大小降得很低。这样结果就是主干几乎不占可训练显存（因为它是“压缩+冻结”的），训练时的显存主要用于 LoRA 的小矩阵、激活、优化器状态等，从而能在一张中等显存卡上训练很大的模型。\n为什么用：让“千亿参数超大模型”能在单块高端GPU上微调。例：千亿模型全微调跑不起来，QLoRA可在单卡A100上完成。\n怎么用：QLoRA 不是把模型“直接丢成 4-bit 然后训练”，而是有很多工程/算法技巧保证性能与可反向传播性，比如NF4（NormalFloat 4-bit）数据类型、Double Quantization（双重量化）、Paged Optimizers（分页优化器 / 内存调度）等等，这里不在赘述，可以参考QLoRA（Quantized LoRA）详解。Qlora最明显的优点就是显存暴降；但同时也会有缺点：量化损失少量精度，但对效果影响极小。\n\n\n2.4.4 Prompt Tuning（提示微调）\n是什么：不怎么动模型参数，仅训练“提示（Prompt）”参数，给任务加一段特殊“前缀提示”，让模型通过学习前缀适配任务。Prompt Tuning是发生在Embedding这个环节的。如果将大模型比做一个函数：Y=f(X)，那么Prompt Tuning就是在保证函数本身不变的前提下，在X前面加上了一些特定的内容，而这些内容可以影响X生成期望中Y的概率。 从数学的角度来分析：Prompt Tuning 的核心是训练 “可学习的嵌入向量（Soft Prompt）” 作为输入前缀，而非直接拼接文本提示。这些嵌入向量与模型的词嵌入维度一致（如 768 维），会被拼接在原始输入的嵌入层之前，引导模型适配任务。例如，训练一组形如 [P1, P2, P3, P4] 的可学习嵌入（每个 P 是一个向量），拼接在 “这部电影很有趣。总体评价是 [MASK] 的” 之前，让模型学习用这些嵌入向量捕捉任务模式。它本质是监督微调的一种 “参数高效变体”—— 训练数据与普通 SFT 一致（“指令 - 标注输出” 对），但仅更新 “提示嵌入” 的参数（通常仅占总参数的 0.1%~1%），模型主体完全冻结。\n为什么用：Prompt Tuning的出发点，是基座模型(Foundation Model)的参数不变，为每个特定任务，训练一个少量参数的小模型，在具体执行特定任务的时候按需调用，由于参数更新量极小这个特点，适合“多任务快速切换”（电商问答—&gt;医疗问答，换Prompt即可）。\n怎么用：首先构造输入，将可学习的 “提示嵌入” 与原始输入的嵌入层拼接。例如，输入文本 “特效很棒，但剧情糟糕。” 的嵌入会与训练好的 [P1, P2, P3, P4] 嵌入拼接，形成完整输入。冻结预训练模型所有参数，仅更新 “提示嵌入” 的参数。用 SFT 数据（“指令 - 标注输出” 对）计算损失（如掩码预测的交叉熵），反向传播时仅优化提示嵌入。训练完成后，同一任务的所有查询都使用这套 “提示嵌入”。缺点：复杂任务效果明显弱于LoRA/全微调\n\n\n2.4.5 Prefix Tuning（前缀微调）\n是什么：出发点类似Prompt Tuning，但具体实现上与前者在Embedding环节往输入序列X前面加特定的Token不同，Prefix Tuning是在Transformer的Encoder和Decoder的网络中都加了一些特定的前缀。训练“连续前缀向量”（不是文本，而是设计的模型能理解的向量），对输入影响更精细。\n为什么用：比Prompt Tuning更灵活，效果略好，且仍保持“参数效率高”。\n怎么用：在模型注意力q，v等权重向量前加“可训练前缀向量”，用SFT数据训练向量，模型主体不动。适合想提升Prompt Tuning效果、又不想多训参数的场景。\n\n\n2.4.6 Adapter Tuning（适配器微调）\n是什么：在预训练模型各层之间插入“小型适配器网络”（如几个全连接层），仅训练适配器。\n为什么用：对主模型干扰小（主参数冻结），同时适配器能针对性学任务特征；推理时仅增加少量延迟。\n怎么用：在模型层（如Transformer编码器层）间插入Adapter，用SFT数据训练Adapter参数。适合既要保主模型通用能力、又要适配任务的场景。\n\n\n2.4.7 BitFit（偏置微调）\n是什么：仅训练模型的“偏置（Bias）”参数（可理解为“辅助调节开关”），其他参数全冻结。\n为什么用：参数更新量极小，训练超快，适合“快速验证任务可行性”。\n怎么用：仅开放模型中所有Bias参数的训练权限，用SFT数据训练。缺点：复杂任务效果一般，适合简单任务/前期试错。\n\n三、对齐训练（Alignment Training）3.1 什么是对齐训练？有监督微调（SFT）让模型“会做事”，但它并不保证模型“做得对”。对齐训练（Alignment Training） 的目标是进一步让模型的输出符合人类的价值观、审美与行为准则，确保它在各种复杂场景中既“有能力”又“有边界”。\n简单来说，对齐是让模型从：\n\n“我知道答案” → “我知道人类希望我怎么回答”。\n\n因此，Alignment 是大语言模型迈向可信与可控智能的关键环节。它的核心追求是构建一个有益（Helpful）、诚实（Honest）、无害（Harmless）的模型，也被称为 “3H 原则”。\n\n3.2 为什么要对齐？预训练与SFT只能让模型学习语言规律与任务执行，但无法保证：\n\n输出是否符合伦理与价值观（如不得生成攻击性言论）；\n回答是否符合事实（防止“自信地胡说”）；\n行为是否符合用户意图（避免曲解、答非所问）；\n是否具备稳定的风格与安全边界。\n\n如果缺乏对齐训练，模型可能出现以下问题：\n\n生成有害或偏见性内容（toxicity, bias）；\n出现“幻觉”（hallucination）；\n不服从人类指令（non-compliance）；\n行为不一致，导致用户信任度下降。\n\n因此，对齐训练是连接 技术能力 与 人类价值 的关键桥梁，使模型输出更贴近人类偏好和社会规范。\n\n3.3 对齐的主要实现方式目前主流的对齐训练方式主要包括三种范式：\n\n\n\n\n对齐范式\n核心思想\n优点\n代表模型\n\n\n\n\n基于人类反馈的强化学习（RLHF）\n通过奖励模型学习人类偏好，用强化学习优化LLM行为\n效果成熟、业界标准\nChatGPT、Claude、PaLM\n\n\n直接偏好优化（DPO）\n直接将人类偏好数据转化为损失函数微调LLM，无需训练奖励模型\n训练简单、稳定性高\nLLaMA3、Mistral、Yi系列\n\n\n基于AI反馈的强化学习（RLAIF）\n由强大的教师模型替代人类提供反馈，实现自动化对齐\n成本低、可规模化\nGemini、GPT-4-turbo 系列\n\n\n\n\n下面重点介绍前两种核心范式：RLHF 和 DPO。\n\n3.3.1 RLHF（Reinforcement Learning from Human Feedback）（1）核心思想RLHF 目标是让模型输出更符合人类偏好，通过“人类打分”来引导模型行为的强化学习闭环。其典型流程分为三步：\n\nSupervised Fine-Tuning（SFT）在预训练模型上使用人工标注的高质量指令数据进行监督微调，使模型具备初步的指令理解能力。\n\nReward Model（RM）训练\n\n收集模型对同一指令生成的多个回答；\n由人类标注者比较这些回答的好坏（如选出“更符合人类偏好”的答案）；\n用这些对比数据训练一个奖励模型 ( $R_\\phi$ )，学会预测人类更倾向哪个输出。\n\n换言之，RM 学的是人类“偏好函数”。\n\n强化学习优化（如PPO算法）\n\n使用 RM 的评分作为奖励信号；\n以语言模型 ( $\\pi_\\theta$ ) 为智能体，优化其生成策略，使高分回答概率更大；\n常采用 PPO（Proximal Policy Optimization） 进行训练，以防止模型输出分布漂移过大导致“崩坏”。\n\n\n\n整个流程如下：\n\nSFT → 奖励模型（RM） → 强化学习优化（PPO）\n\n示意图如下：\n\n（2）奖励模型的关键要素\n输入：prompt + candidate responses；\n输出：一个实数分数 ( $R_\\phi(x, y) $)，代表该回答被人类偏好的程度；\n损失函数：通常采用 pairwise ranking loss，使 ($ R_\\phi(y_{preferred}) &gt; R_\\phi(y_{rejected})$ )。\n\n（3）PPO强化学习阶段\n优化目标：\n\n [\n \\max_\\theta \\mathbb{E}*{x,y\\sim \\pi*\\theta} [R_\\phi(x, y) - \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{SFT}}(y|x)}]\n ]其中第二项为KL正则项，用于约束新模型不偏离SFT模型。\n\n\n\n（4）优缺点\n\n\n\n优点\n缺点\n\n\n\n\n输出质量高、行为自然、偏好一致性好\n成本高（需大量人工反馈）、训练复杂（RM+PPO）、稳定性较难控制\n\n\n\n\n尽管 RLHF 能充分利用人类偏好信息提升模型对齐效果，但其固有局限性包括：\n\n多模型训练：除 Actor 模型外，还需额外训练奖励模型$R_\\phi$和 Critic 模型，整体训练过程复杂且资源消耗大。\n高采样成本：LLM 生成文本计算量大，强化学习过程中的大量在线采样进一步推高了计算开销；采样不足可能导致错误的优化方向。\n训练不稳定与超参数敏感：PPO 涉及众多超参数（如学习率、采样量等），调参复杂且训练过程易受不稳定因素影响。\n对齐税效应：在提高模型对齐性的同时，可能会降低模型在其他任务上的表现。\n\n3.3.2 DPO（Direct Preference Optimization）RLHF 虽然效果好，但流程复杂，成本高昂并且训练不稳定，对超参数敏感。为此，研究者提出了更简化的方案 —— DPO（直接偏好优化）。\n（1）核心思想DPO 不再训练独立的奖励模型，也不使用强化学习算法，而是直接利用偏好对比数据定义一个新的损失函数来优化语言模型，其核心思路是将 RLHF 的目标转化为类似于监督微调的对比学习任务\n训练数据形式仍是「偏好对」：\n(prompt, chosen_response, rejected_response)\n即人类认为 “chosen” 比 “rejected” 更好。\nDPO 通过一个基于对数几率的目标函数，让模型倾向于生成“chosen”样本，而避免“rejected”样本，公式如下：\n\n[\n L_{\\text{DPO}}(\\theta) = -\\log \\sigma\\Big(\\beta \\cdot [\\log \\pi_\\theta(y_c|x) - \\log \\pi_\\theta(y_r|x) - \\log \\pi_{\\text{ref}}(y_c|x) + \\log \\pi_{\\text{ref}}(y_r|x)]\\Big)\n ]其中：\n\n( $\\pi_\\theta$ )：当前待优化模型；\n( $\\pi_{\\text{ref}} $)：SFT后的参考模型；\n( $\\beta $)：温度系数，调控对齐强度；\n( $y_c$, $y_r$ )：分别为“优选”与“弃选”回答。\n\n\n（2）优点\n无需训练独立RM，直接微调LLM本体；\n训练稳定、收敛快；\n可无缝结合LoRA等轻量化方案；\n效果接近甚至优于RLHF。\n\n（3）变体：LoRA-DPO在显存有限的情况下，可结合 LoRA（Low-Rank Adaptation） 技术，仅对少量参数进行低秩更新，实现：\n\n“轻量参数 + 高效对齐” 的平衡。LoRA-DPO 已成为开源模型（如 LLaMA3、Yi-1.6、Qwen1.5）的主流对齐方式。\n\n\n3.4 小结\n\n\n\n阶段\n目标\n代表方法\n特点\n\n\n\n\nSFT\n让模型学会执行任务\n有监督微调\n教“会不会做”\n\n\nRLHF\n让模型更符合人类偏好\nRM + PPO\n教“做得对不对”\n\n\nDPO\n用更简洁的方式实现对齐\n直接偏好优化\n教“做得更好、更稳定”\n\n\n\n\n四、场景应用与实战指南前文我们已经系统地了解了预训练、SFT和对齐训练的理论知识。但在实际项目中，我们面临的第一个问题往往是：“我的任务到底需不需要微调？需要哪种微调？” 本节将聚焦于这些实际问题，提供场景化的决策参考和一个简化的实战流程。\n4.1 SFT 场景辨析：何时必须做？SFT（有监督微调）并非总是必须的。 对于一些简单的、通用的任务，强大的基础模型（如 GPT-4, Llama 3）通过精心设计的 Prompt Engineering 或 少样本学习（Few-shot Learning） 就可能获得足够好的效果，这样做成本最低、速度最快。\n那么，什么场景下 SFT 是强烈推荐甚至必须的呢？\n场景一：领域知识注入-Domain Adaptation当你的任务需要模型掌握特定领域的专业知识、术语或内部“黑话”时，SFT是必选项。\n\n问题：通用模型不了解你公司的产品、内部流程或特定行业的专业术语。\n\n例子：\n\n医疗领域：让模型能看懂病历，并根据《XX疾病诊断指南》生成摘要。通用模型可能不认识某些药物名称或诊断缩写。\n金融领域：让模型担任投研助手，它必须理解“风险敞口”、“阿尔法收益”等专业词汇，并能解析特定公司的财报。\n企业内部：让模型成为新员工的入职助手，需要回答“如何申请‘星辰计划’项目经费？”这类公司内部问题。\n\n\nSFT 作用：通过包含这些专业知识的指令数据进行微调，相当于给模型“补课”，让它成为该领域的“专家”。\n\n\n场景二：特定任务格式遵循-Task Format Adherence当你的任务要求模型严格按照某种特定格式输出时，SFT的效果远胜于复杂的Prompt。\n\n问题：Prompt有时不稳定，模型可能“自由发挥”，无法保证每次都输出你想要的格式（如 JSON、Markdown 表格、XML等）。\n\n例子：\n\n信息抽取：从一段非结构化文本中，抽取出人名、公司和职位，并以固定的 JSON 格式 返回。\n代码生成：要求模型根据需求，生成符合公司内部编码规范（如特定的变量命名、注释风格）的代码。\n客服摘要：将一段长对话自动总结为包含“客户诉求”、“解决方案”、“待办事项”三个字段的报告。\n\n\nSFT 作用：通过大量格式正确的“输入-输出”对进行训练，让模型形成“肌肉记忆”，从而稳定、可靠地生成所需格式。\n\n\n场景三：特定风格/人设模仿-Style/Persona Imitation当需要模型以一种独特的语气、风格或人设进行交流时，SFT是塑造其“性格”的最佳方式。\n\n问题：通用模型的回应风格通常是中立、客观的“AI助手”风格，不符合品牌或产品定位。\n\n例子：\n\n儿童教育产品：需要模型扮演一个活泼、可爱、有耐心的“故事大王”角色。\n法律咨询助手：需要模型使用严谨、正式、专业的法律术语进行回复。\n品牌营销文案：需要模型模仿特定品牌（如“杜蕾斯”）的犀利、幽默风格。\n\n\nSFT 作用：SFT 数据集中的 output 不仅是正确答案，也承载了期望的风格。模型在学习答案的同时，也学会了“如何说”。\n\n\n\n🎯 决策小结：\n\n如果你的任务用 Prompt 就能稳定解决，那就别做 SFT。\n如果涉及专业知识、固定格式、特定人设，且 Prompt 效果不稳定或成本太高，SFT 就是你的最佳选择。\n\n\n4.2 指令数据的多样性：从单轮到多轮与CoT准确的来说监督微调，可以包含指令微调（明确任务指令，如翻译）对话微调（多轮对话数据，如客服）领域适配（特定领域术语，如医疗）⽂本分类（结构化标签，如情感分析）模型推理微调（思维链标注，如数学解题）\n除了前文提到的 instruction-input-output 结构，指令对的形式也需要根据任务需求进行设计，以激发模型不同的能力。\n1. 单轮指令（Single-Turn Instruction）最基础的形式，适用于简单的问答、分类、摘要等一次性完成的任务。\n&#123;  &quot;instruction&quot;: &quot;将下面这句话翻译成英文。&quot;,  &quot;input&quot;: &quot;今天天气真不错。&quot;,  &quot;output&quot;: &quot;The weather is really nice today.&quot;&#125;\n2. 多轮对话（Multi-Turn Dialogue）用于构建能理解上下文的对话机器人。数据集需要包含完整的对话历史，让模型学会联系上下文进行回复。\n\n格式：通常采用类似 ChatML 的格式，用角色（system, user, assistant）来区分对话内容。\n例子：\n\n[  &#123;    &quot;from&quot;: &quot;human&quot;,    &quot;value&quot;: &quot;你好，帮我推荐一款适合新手的笔记本电脑，预算5000元左右。&quot;  &#125;,  &#123;    &quot;from&quot;: &quot;gpt&quot;,    &quot;value&quot;: &quot;好的，这个预算内，您可以考虑联想小新Pro 14或者小米笔记本Pro 15。您更看重轻薄便携还是屏幕素质呢？&quot;  &#125;,  &#123;    &quot;from&quot;: &quot;human&quot;,    &quot;value&quot;: &quot;我经常出差，所以轻薄便携更重要。&quot;  &#125;,  &#123;    &quot;from&quot;: &quot;gpt&quot;,    &quot;value&quot;: &quot;明白了。那么联想小新Pro 14会更适合您，它的重量只有1.3kg，非常便于携带。性能也完全满足日常办公和影音娱乐。&quot;  &#125;]\n\n训练目标：模型在生成第二个&quot;gpt&quot;回答时，必须同时考虑两个&quot;human&quot;的输入（预算和便携性要求）。\n\n3. 思维链（Chain of Thought, CoT）用于提升模型在逻辑推理、数学计算、复杂规划等任务上的能力。CoT 的核心是不仅告诉模型“答案是什么”，更要教它“如何一步步得到答案”。\n\n格式：在 output 中，先包含详细的思考过程，再给出最终答案。\n例子：\n\n&#123;  &quot;instruction&quot;: &quot;农场里有10只鸡和8只兔子，请问它们一共有多少条腿？&quot;,  &quot;input&quot;: &quot;&quot;,  &quot;output&quot;: &quot;好的，我们来一步步计算：\\n1. 首先计算鸡的腿数：鸡有2条腿，所以10只鸡一共有 10 * 2 = 20 条腿。\\n2. 接着计算兔子的腿数：兔子有4条腿，所以8只兔子一共有 8 * 4 = 32 条腿。\\n3. 最后将它们的腿数相加：20 + 32 = 52 条腿。\\n所以，它们一共有52条腿。&quot;&#125;\n\n训练目标：通过学习 CoT 数据，模型能够学会将复杂问题分解为简单步骤，并按逻辑顺序求解，从而大幅提高推理任务的准确率。\n\n4.3 对齐训练场景辨析：DPO/RLHF 是必需品吗？与SFT类似，对齐训练（如DPO）也并非总是必须的。\n场景一：对齐训练是强需求的当你的模型需要直接面向大量、不可控的公众用户时，对齐几乎是必备环节。\n\n问题：SFT 后的模型虽然“能干活”，但可能“乱说话”。它可能生成有害、偏见、不安全或虚假的内容，引发严重的品牌或法律风险。\n\n例子：\n\n公共聊天机器人（如文心一言、ChatGPT）：必须确保对敏感问题（政治、宗教、暴力等）有安全、无害的回复。\nAI 心理咨询助手：必须避免给出有害或不负责任的建议，学会共情和提供积极引导。\n高风险决策辅助（如医疗诊断建议）：模型需要诚实地表达不确定性（“我无法确定，请咨询专业医生”），而不是“自信地胡说八道”（幻觉）。\n\n\nDPO/RLHF 作用：通过人类偏好数据，为模型建立一套“价值观”，教会它在复杂的道德和安全边界问题上做出“正确”的选择。DPO 更像是在给模型划定“红线”和“底线”。\n\n\n场景二：对齐训练需求较弱或非必需的\n内部工具或后端服务：如果模型仅用于公司内部，由专业人士使用（如程序员用它生成代码、研究员用它分析数据），用户有能力辨别输出的质量，那么对齐的优先级就较低。\n高度受控的特定任务：如果模型只执行一个非常狭窄的任务（如从财报中提取营收数字），其输入和输出都高度可控，产生有害内容的风险极低。\nSFT 数据质量极高：如果你的 SFT 数据本身就已经蕴含了强烈的偏好信号（即所有 output 都是非常优质、安全、负责的回答），那么在某种程度上，SFT 本身也起到了“隐式对齐”的作用。\n\n\n🎯 决策小结：\n\n面向公众、开放域、高风险的应用，必须做对齐。\n内部使用、窄领域、低风险的应用，可以优先考虑不做或延后做，以节约成本。\n\n\n4.4 案例：从通用模型到垂直领域专家让我们通过一个金融客服助手的例子，串联起整个流程。\n\n阶段一：选择基础模型（预训练）\n\n目标：需要一个懂中文、有基本对话能力的模型。\n选择：Llama-3-8B-Instruct 或 Qwen1.5-7B-Chat。这些模型已经完成了预训练，具备了强大的通用语言能力。\n现状：此时的模型像一个“什么都懂一点，但什么都不精”的通才。你问它“什么是市盈率？”，它能答对；但你问“我们公司的‘金卡理财’产品年化收益率是多少？”，它会说“我不知道”。\n\n\n阶段二：领域知识与能力微调（SFT）\n\n目标：让模型成为一名合格的“XX银行”金融产品客服。\n\n数据准备：\n\n知识型：&quot;instruction&quot;: &quot;介绍一下我们的‘稳健增长’基金&quot;, &quot;output&quot;: &quot;‘稳健增长’基金是...&quot;\n任务型（多轮）：构建用户咨询、查询余额、办理业务的模拟对话。\n风格型：所有 output 都采用礼貌、专业、严谨的客服话术。\n\n\n训练：使用 QLoRA 在 SFT 数据集上进行微调。\n\n结果：得到一个 Finance-LLM-SFT 模型。它现在能准确回答公司产品问题，并以客服的口吻交流。但当你问它“我应该把所有钱都投到股票里吗？”，它可能会给出不负责任的建议。\n\n\n\n阶段三：安全与价值观对齐（DPO）\n\n目标：确保模型不提供投资建议、不泄露隐私、回答安全可靠。\n\n数据准备（偏好对）：\n\nPrompt: “我应该把所有钱都投到股票里吗？”\nChosen (更优): “作为AI助手，我无法提供投资建议。投资有风险，建议您咨询专业的理财顾问。”\nRejected (更差): “是的，股市回报高，值得一试！”\n\n\n训练：在 Finance-LLM-SFT 模型基础上，用偏好数据集进行 DPO 训练。\n\n结果：得到最终的 Finance-LLM-SFT-DPO 模型。它不仅是业务专家，更是一个安全、可靠、值得信赖的金融助手。\n\n\n\n\n4.5 微调实战入门指南（以 QLoRA SFT 为例）这里提供一个使用 Hugging Face 生态（transformers, peft, trl）进行 QLoRA 微调的简化步骤，让你对整个流程有体感。\n第一步：环境准备安装必要的库。bitsandbytes 是量化所必需的。\npip install torch transformers datasets peft accelerate bitsandbytes trl\n第二步：选择基础模型和分词器选择一个合适的开源模型，比如 Qwen1.5。\nimport torchfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfigmodel_name = &quot;Qwen/Qwen1.5-7B-Chat&quot;# 配置4-bit量化quantization_config = BitsAndBytesConfig(    load_in_4bit=True,    bnb_4bit_compute_dtype=torch.bfloat16,    bnb_4bit_quant_type=&quot;nf4&quot;)# 加载量化后的模型和分词器model = AutoModelForCausalLM.from_pretrained(    model_name,    quantization_config=quantization_config,    device_map=&quot;auto&quot; # 自动分配到GPU)tokenizer = AutoTokenizer.from_pretrained(model_name)tokenizer.pad_token = tokenizer.eos_token # 设置pad_token\n第三步：准备和处理数据集假设你有一个 JSONL 格式的数据集 my_dataset.jsonl，每行是一个对话。\n&#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;&#125;, &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;你好！有什么可以帮助你的吗？&quot;&#125;]&#125;&#123;&quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;请用一句话总结下这篇新闻。&quot;&#125;, &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;科技公司发布能效提升50%的新AI芯片。&quot;&#125;]&#125;\n使用 datasets 库加载，并应用聊天模板。\nfrom datasets import load_dataset# 加载数据集dataset = load_dataset(&quot;json&quot;, data_files=&quot;my_dataset.jsonl&quot;, split=&quot;train&quot;)# 定义一个处理函数，将对话格式化为模型输入def formatting_prompts_func(example):    output_texts = []    for i in range(len(example[&#x27;messages&#x27;])):        text = tokenizer.apply_chat_template(example[&#x27;messages&#x27;][i], tokenize=False, add_generation_prompt=False)        output_texts.append(text)    return output_texts\n第四步：配置 LoRA 和训练参数使用 peft 配置 LoRA，使用 transformers 配置训练参数。\nfrom peft import LoraConfig, get_peft_modelfrom transformers import TrainingArguments# LoRA 配置lora_config = LoraConfig(    r=8,  # 低秩矩阵的秩    lora_alpha=16,    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;], # 对注意力模块应用LoRA    lora_dropout=0.05,    bias=&quot;none&quot;,    task_type=&quot;CAUSAL_LM&quot;)# 应用PEFTmodel = get_peft_model(model, lora_config)# 训练参数training_args = TrainingArguments(    output_dir=&quot;./qwen-7b-sft&quot;,    per_device_train_batch_size=4,    gradient_accumulation_steps=4,    learning_rate=2e-4,    num_train_epochs=3,    logging_steps=10,    save_steps=100,    fp16=True, # 使用混合精度训练)\n第五步：开始训练使用 trl 库的 SFTTrainer，它极大地简化了指令微调的流程。\nfrom trl import SFTTrainertrainer = SFTTrainer(    model=model,    args=training_args,    train_dataset=dataset,    packing=True, # 将多个短样本打包成一个长样本，提高效率    formatting_func=formatting_prompts_func, # 使用我们定义的数据处理函数    max_seq_length=1024,)# 开始训练trainer.train()# 保存LoRA适配器trainer.save_model(&quot;./qwen-7b-sft-lora&quot;)\n第六步：推理测试加载基础模型和训练好的 LoRA 适配器进行测试。\nfrom peft import PeftModel# 加载基础模型base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=&quot;auto&quot;)# 加载LoRA权重model = PeftModel.from_pretrained(base_model, &quot;./qwen-7b-sft-lora&quot;)# 合并模型（可选，合并后推理更快，但无法再卸载LoRA）# model = model.merge_and_unload()# 测试prompt = &quot;请用一句话总结下这篇新闻。&quot;messages = [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;]text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)inputs = tokenizer(text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)outputs = model.generate(**inputs, max_new_tokens=50)print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n这个实战指南只是一个起点和示例，目前是用代码的方式来实现，当然还有零代码就能进行微调的LLamA-Factory，实际项目中还需要关注数据清洗、超参调优、模型评估等更多细节。\n","categories":["学习"],"tags":["大模型"]}]